[["index.html", "industRial data science Welcome", " industRial data science Case studies in product development and manufacturing João Ramalho 2021-10-23 Welcome This is the online version of industRial data science, a book with tools and techniques for data analysis in Product Development and Manufacturing. It is organized around Case Studies in a “cookbook” approach, making it easier to directly adopt the tools. The examples come from varied manufacturing industries, mostly where repetitive production in massive quantities is involved, including: pharmaceuticals, food, electronics, watch making and automotive. Product Development and Manufacturing are very important activities in society because bringing innovative products to the market has an immense potential to improve the quality of life of everyone. Additionally Data Science brings new powerful approaches to the engineering and manufacturing of consumer goods, helping minimizing environmental impact, improving quality and keeping costs under control. How to use this book We assume the reader is familiar with product development and manufacturing quality methodologies such as dmaic and six sigma and the related statistical concepts. We also assume the reader is already a user of the R programming language. The Case Studies then bring all the areas together in a practical way. This book is better used as a reference book by using the navigation bar on the left to go a specific industrial domain. To reproduce the examples all the case studies data sets, example functions and the textbook original files can be downloaded as a package called {industRial}. For guidelines on how to use them refer to the sections Datasets and Functions Complementing the text, a series of tutorials can be accessed either online or locally to practice dynamically key statistical concepts. For the online option no specific software installation is required. A list of web links and detailed instructions on local installation can be seen in the section Tutorials. In the appendix we provide a detailed Index and a Glossary and we refer to several good quality books on both Data Science and Product Development that have served to provide the required theoretical background. These cover key disciplines such as six sigma, statistics and computer programming. This book aims complementing them and showcase how to benefit from recent software in this area. A full list can be found in References. Content overview The case studies are organized according the a logical product development flow. The text starts with case studies in the domain of Design for Six Sigma. These are some practical tools in that help prioritizing problems and get focus on how to tackle them. The next group of case studies covers the domain of Measurement System Analysis, an initial important step when developing a product or manufacturing process. Here is discussed how to analyze the response of a measurement device in terms of its bias and its uncertainty. The next large group of case studies is the Design of Experiments. This corresponds to the core of the R&amp;D activities and provides approaches to minimize the quantity of trials and time to reach to a sufficient knowledge of how the product or system works. Also on how to obtain the right balance on its features and properties to get the desired output. A final group of case studies presents ways to get the manufacturing process in control according to what was defined in the product development phase. These are the well known Statistical Process Control and Capability studies. Acknowledgements I would like to express my gratitude to the instructors and colleagues who have spent time sharing their knowledge, answering my questions and giving me inputs: Enrico Chavez, Iegor Rudnytskyi, Giulia Ruggeri, Harry Handerson and Bobby Stuijfzand from the EPFL ADSCV team; Jean-Vincent Le Bé, Jasmine Petry, Yvan Bouza, James Clulow and Akos Spiegel from the Nestlé STC team; Frank Paris from DOQS; Théophile Emmanouilidis and Sélim Ach from Thoth. To report any issue or make suggestions please open an issue on the book repository: industRialds/issues About the authors João Ramalho is a Senior Industrial Data Scientist with more than 20 years of experience in the manufacturing industry. He’s been in varied positions in R&amp;D, Operations and IT at Philip Morris, Rolex and Nestlé. He holds a Master in Mechanical Engineering from the IST of Lisbon, a PMP certification from the Project Management Institute and a Data Science certification from DataCamp. He’s currently specializing in Data Visualization at the Swiss technical university EPFL. See full profile at j-ramalho.github.io "],["toolbox.html", "Toolbox R and RStudio Packages Datasets Functions", " Toolbox R and RStudio Many tools exist to do Data Analysis and Statistics with different degrees of power and difficulty such as: Spreadsheets: Excel, Libreoffice, Numbers Proprietary software: Minitab, Matlab Programming languages: Visual Basic, R, Python, Julia Databases: sqlite, postgre, mysql, mongodb Choosing the right set of tools for Data Science is often not a very scientific task. Mostly is a matter of what is available and what our colleagues, customers or suppliers use. As with everything it is important to remain open to evaluate new tools and approaches and even to be able to combine them. In this book we’ve chosen to provide all examples in R which is a free software environment for statistical computing and graphics. Besides taste and personal preference R brings a significant number of specific advantages in the field of Industrial Data Science: R allows for reproducible research This because the algorithms and functions defined to make the calculations can be inspected and all results can be fully reproduced and audited. This is known as reproducible research and is a critical aspect in all areas where a proof is needed such as in equipment validation and product quality reporting. R functions and tools can be audited and improved Being an open source language, all R libraries and packages added to the basic environment can be audited, adapted and improved. This is very important because when we enter into details every industry has a slight different way of doing things, different naming conventions, different coefficients and so on. R is extensible R is compatible with most other software on the market and is an excellent “glue” tool allowing for example for data loading from excel files, producing reports in PDF and even building complete dashboards in the form of web pages. Large documentation is available on installing and learning R, starting with the official sites R-project and RStudio. Packages All industry specific tools applied throughout this book are available in the form of packages of the programming language R. As with all open source code, they’re all available for download with freedom to modification and at no cost. The amount of packages available is extremely large and growing very fast. When selecting new packages it is recommended to check the latest package update. Packages that have had no improvements since more than a couple of years should be questioned. The field evolves rapidly and compatibility and other issues can become painful. Two ways of obtaining statistics on package history are metacran and RStudio package manager. Additionally an original package named {industRial} has been developed as a companion package for this book. Installation The companion package to this book can downloaded from CRAN with: install.packages(&quot;industRial&quot;) A development version is also available from github with: devtools::install_github(&quot;J-Ramalho/industRial&quot;) The list below identifies which are the remaining packages required to run all the coded of the book examples. Note that it is not required to download them all at once. The column Download precises if a package is downloaded automatically when the {industRial} package is downloaded (imports) or if it needs be downloaded manually by the reader (suggests). In technical terms this differentiation corresponds to the DESCRIPTION file and allows for a progressive installation of the required software. Download Package Domain Version Depends imports viridis colors 0.6.1 R (&gt;= 2.10), viridisLite (&gt;= 0.4.0) imports readr data loading 1.4.0 R (&gt;= 3.1) imports dplyr data wrangling 1.0.6 R (&gt;= 3.3.0) imports forcats data wrangling 0.5.1 R (&gt;= 3.2) imports janitor data wrangling 2.1.0 R (&gt;= 3.1.2) imports magrittr data wrangling 2.0.1 NA imports tibble data wrangling 3.1.2 R (&gt;= 3.1.0) imports tidyr data wrangling 1.1.3 R (&gt;= 3.1) imports purrr data wrangling 0.3.4 R (&gt;= 3.2) imports glue data wrangling 1.4.2 R (&gt;= 3.2) imports stringr data wrangling 1.4.0 R (&gt;= 3.1) imports patchwork plotting 1.1.1 NA imports ggplot2 plotting 3.3.3 R (&gt;= 3.2) imports scales plotting 1.1.1 R (&gt;= 3.2) imports ggtext plotting 0.1.1 R (&gt;= 3.5) imports broom statistics 0.7.6 R (&gt;= 3.1) imports stats statistics 4.1.1 NA imports skimr statistics 2.1.3 R (&gt;= 3.1.2) suggests qicharts2 industrial stats 0.7.1 R (&gt;= 3.0.0) suggests qcc industrial stats 2.7 R (&gt;= 3.0) suggests SixSigma industrial stats 0.9-52 R (&gt;= 2.14.0) suggests DoE.base industrial stats 1.1-6 R (&gt;= 2.10), grid, conf.design suggests rsm industrial stats 2.10.2 NA suggests agricolae industrial stats 1.3-3 R (&gt;= 2.10) suggests ggraph networks 2.0.5 R (&gt;= 2.10), ggplot2 (&gt;= 3.0.0) suggests tidygraph networks 1.2.0 NA suggests igraph networks 1.2.6 methods suggests ggforce networks 0.3.3 ggplot2 (&gt;= 3.0.0), R (&gt;= 3.3.0) suggests bookdown publishing 0.22 NA suggests knitr publishing 1.33 R (&gt;= 3.2.3) suggests kableextra publishing NA NA suggests gt publishing 0.3.0 R (&gt;= 3.2.0) suggests car statistics 3.0-10 R (&gt;= 3.5.0), carData (&gt;= 3.0-0) suggests RcmdrMisc statistics 2.7-1 R (&gt;= 3.5.0), utils, car (&gt;= 3.0-0), sandwich In the book text we don’t see the loading instructions for the installed packages over and over again every time an example is given to avoid repetition (e.g. running library(dplyr) before each code chunk). Be sure to load at minimum the packages below before trying any example: ds_pkgs &lt;- c(&quot;tidyverse&quot;, &quot;scales&quot;, &quot;janitor&quot;, &quot;knitr&quot;, &quot;stats&quot;, &quot;industRial&quot;, &quot;viridis&quot;, &quot;broom&quot;, &quot;patchwork&quot;) purrr::map(ds_pkgs, library, character.only = TRUE) Beware of the common issue of function masking. This happens more often in R when compared to python. As we tend to load all the sets of functions from each package we end up with conflicting function names. In the scope of this text it is mostly the function filter() from {dplyr} which conflicts with the function with the same name from {stats}. We tackle this with the simple technique of adding filter &lt;- dplyr::filter in the beginning of our script to precise which function we want to give priority and we pre-append the package name to all calls of the other function such as stats::filter. For more sophisticated ways to handle this issue we suggest the package {import}. Highlights We’re highlighting now some specific packages that are used in the book and that bring powerful features in analysis of data from R&amp;D and Operations. Wherever they are required in the book we loaded them explicitly in the text to help tracking where the specific functions come from. six sigma {SixSigma} is a complete and robust R package. It provides many well tested functions in the area of quality and process improvement. We’re presenting a full example with the gage r&amp;R function in our MSA Case Study. As many other industrial packages, the {SixSigma} package is from before the {tidyverse} era and its plots have not all been been developed with {ggplot2}. This sometimes makes integration in newer approaches a little bit harder. The data output is still nevertheless fully exploitable and very useful. The package is part of an excellent book with the same name published by Emilio L. Cano (2012). qcc The Quality Control Charts package, {qcc} is another very complete and solid package. It offers a very large range of statistical process control charts and capability analysis. Several examples of the control charts are available in its vignette: qcc vignette that we develop further in our SPC Case Studies. qicharts2 We recommend {qichart2} specifically for the good pareto plots. The package also provides statistical process control charts which are based on {ggplot2} and can serve as an easier alternative to the {qcc} package. As many niche packages we need to be aware that the number of contributors is small meaning that it cannot be as thoroughly tested as community packages. DoE.base This package is one of the most complete and vast packages in Design of Experiments. {DoE.base} is a first of a large suite of packages on the topic, it has vast functionality and is very well documented. We do some exploration of the automatic generation of designs in the DOE case studies. Full documentation available under: DoE.base agricolae Agricolae is a long tested package in the domain of design of experiments. It has been developed for the domain of Agricultural Research but can be used elsewhere. We make a small use, specifically to obtain the function fisher LSD but believe the package has a wealth of functions and methodologies to be explored. rsm The Response Surface Methods {rsm} is the best option in our view to produce 3D plots from linear models. It has specific features to use directly the models removing all the work to produce the data and feed generic 3D plotting functions. The package is larger than this and contains many support functions in the domain of design of experiments. car The {car} package which stands for Companion for Applied Regression is also used in many occasions as it contains many useful functions to assess the performance of linear models and anova. This package is combined with a complete book by Fox and Weisberg (2019). RcmdrMisc This package by the same author of the {car} provides additional miscellaneous functions for statistical analysis. Although it is part of a point and click interface for R we value it for its functions and plots in the domain of linear regression. broom The mission of {broom} is to Convert statistical objects into tidy tibbles. This is quite useful when we want to reuse the output of the statistical analysis such as the R-squared in data pipelines with {tidyverse} packages. It becomes specially handy to obtain printing quality outputs in {Rmarkdown} documents with tables rendered with {kable} and in {shiny} apps. Several examples are present throughout our book and mostly in the tutorials. skimr This package comes from ropensci a strong and open community supported by large global organizations such as the NASA. {skimr} is an interesting and powerful alternative to the base summary() function. Two main features make it a strong candidate for regular utilization: the first is its tight integrated with {tidyverse} and {knitr} making it possible to integrate it in pipelines, filtering and so on and in Rmarkdown chunks with specific printing arguments; the second is its extensive customization capabilities allowing to add and remove indicators, data types and presentation formats and aggregation levels. stats Many functions from the packages discussed before are built on the large and extremely well tested {stats} package. This package is installed directly with R and consolidates software code that has been improving and tested for decades. As an example the source code of the lm function has close to 1000 lines. The complete package has more than 400 functions that can be listed with library(help = \"stats\") or ls(\"package:stats\") Datasets All datasets presented throughout the book are fully anonymous. Once the package is correctly installed it can be loaded in the R session as usual with the library() function. library(industRial) The primary goal of {industRial} is to make easily available all the data sets from all case studies. We can easily look for a data set by typing industRial:: and tab. The complete list can also be obtained with the snippet below: data(package = &quot;industRial&quot;) %&gt;% pluck(&quot;results&quot;) %&gt;% as_tibble() %&gt;% select(Item, Title) %&gt;% kable() Item Title battery_charging Charging time of a lithium-ion battery. dial_control Collection of visual defects on watch dial production. ebike_hardening Cycles to failure of ebikes frames after temperature treatment. ebike_hardening2 Cycles to failure of ebikes frames after temperature treatment. juice_drymatter Dry matter content of different juices obtained with two different measurement devices. perfume_experiment Correlation matrix of the input variables of an experiment design in perfume formulation. pet_delivery Tensile strength values on PET raw material for the clothing industry. pet_doe A factorial design for the improvement of PET film tensile strength. solarcell_fill Yearly outputs and fills factor of solarcells of different types. solarcell_output Yearly outputs of solarcells of different types. syringe_diameter Production measurements of the inner diameter of syringes barrels. tablet_thickness Thickness measurements of pharmaceutical tablets tablet_weight Weight measurements of pharmaceutical tablets Once the package is loaded the data objects become immediately available in memory and can directly be used in the examples presented or for further exploration. Lets confirm this invoking the first data set: dial_control %&gt;% head() %&gt;% kable() Operator Date Defect Location id Jane 2018.01.31 Indent 3h D2354 Jane 2018.02.02 Indent 3h D2355 Jane 2018.02.02 Indent 4h D2356 Peter 2018.02.02 Indent 10h D2357 Jane 2018.02.03 Scratch 3h D2358 Jane 2018.02.03 Indent 3h D2359 The dateset can be used and manipulated like any other dataset created in the session or loaded otherwise. For example it can be filtered and assigned to a new variable name: dial_peter &lt;- dial_control %&gt;% filter(Operator == &quot;Peter&quot;) dial_peter %&gt;% head(2) %&gt;% kable() Operator Date Defect Location id Peter 2018.02.02 Indent 10h D2357 Peter 2018.02.03 Scratch 4h D2360 Functions Besides the data sets the {industRial} package also contains toy functions to plot Statistical Process Control (SPC) charts. The objective here is to showcase how to build such functions and their scope of application is limited to the book case studies. For complete and robust SPC functions we recommend using the {QCC} package also described below. Additionally the package contains theme functions to print and customize the aesthetics of spc charts and other charts. These themes are built on top of the {ggplot2} by H.Wickham and {cowplot} package by Claus O.Wilke. The main objective is to give the reader a starting point for customization of charts in this domain. A functions can conveniently be accessed on the console with industRial:: and then tab. The complete list of themes and functions can be seen with: lsf.str(&quot;package:industRial&quot;) %&gt;% unclass() %&gt;% as_tibble() %&gt;% kable() value chart_Cpk chart_I chart_IMR expand_formula off_spec process_Cpk process_stats process_stats_table ss.rr.plots theme_industRial theme_qcc For each function a help page is available and can be obtained the same way as any other R data sets, themes and functions with ?&lt;object&gt; (e.g. ?chart_xbar) To go even deeper and get access to all the code, the original book Rmd files are also bundled in the package and can be seen in the book folder. A way to get the exact folder path is: paste0(.libPaths()[1], &quot;/industRial/book&quot;) [1] &quot;/home/joao/R/x86_64-pc-linux-gnu-library/4.1/industRial/book&quot; References "],["tutorials.html", "Tutorials On the web Locally", " Tutorials A set of practical exercises on key concepts presented throughout this book is available either on the web or locally, instructions follow. On the web the tutorials in the list below are published on the shinyapp.io server and can be freely accessed with the links below: Topic/Link Content Pareto chart This tutorial builds on the The dial polishing workshop case study from the Design for Six Sigma chapter, train building pareto charts using the {qichart2} package and explore how playing with different variables gives new insights into apparently simple data collections. DOE Anova This tutorial explores how the p value is calculated by playing with a dynamic anova chart. This exercise is based on the The e-bike frame hardening process of the DOE Interactions chapter. Response Surface This tutorial tests 3D visualization skills by playing with 3D response surface plots and the related interaction plots using the battery_charging dataset and the {rsm} package. Process Capability In this tutorial we can play with the process centering variability and see how this is translated in the process indicators “percentage out of spec” and Cpk. Locally The same set of tutorials can also be run locally which can be convenient as they load faster. This also allows for further exploration as the original tutorial code becomes available. For downloading instructions refer to the packages (#installation) session. Next load the packages: library(industRial) library(learnr) and list the tutorials with: learnr::available_tutorials(package = &quot;industRial&quot;) Available tutorials: * industRial - anova : &quot;industRial practice&quot; - capability : &quot;industRial practice&quot; - pareto : &quot;industRial practice&quot; - surface : &quot;industRial practice&quot; choose a tutorial and run it as follows: learnr::run_tutorial(package = &quot;industRial&quot;, &quot;anova&quot;) The original files are available in the package tutorials folder. Their names correspond to the tutorial names listed before so there is a simple way to open the desired file, e.g.: rstudioapi::navigateToFile( paste0(.libPaths()[1], &quot;/industRial/tutorials/anova/anova.Rmd&quot;) ) "],["design-for-six-sigma.html", "Design for Six Sigma Pareto analysis Root cause analysis Correlations Clustering", " Design for Six Sigma Quality tools have been grouped under varied names and methodologies being Six Sigma one of the most well known and comprehensives ones. The domain is vast as seen in the many tools collected and described in the Six Sigma book by Roderik A.Munro and J.Zrymiak (2015). For this section we’ve selected a few cases that strongly support Measurement System Analysis, Design of Experiments and Statistical Process Control. Beside supporting the remaining sections, they also pretend to showcase how R can be used also for other purposes than data wrangling and visualization in the domain of industrial data science, typically to obtain easily reproducible diagrams. We start with a case on a dial workshop in the watch making industrial where the Pareto chart comes handy. We then move to a dental prosthesis laboratory to see how a simple fish-bone diagram can help pinpoint special causes of the measurement variation of an optical device and we finish we two different approaches on how to optimize experiment execution by assess the correlation between the outputs in order to minimize the parameters to measure. Pareto analysis Case study: dial polishing workshop Watch dials are received from the stamping process and polished before being sent to the final assembly. As part of the autonomous quality control performed by the polishing operators a count of the defects observed on the dials each day is logged in a spreadsheet. The Pareto chart has always proven an effective way of defining priorities and keeping workload under control. It is known for helping focusing on the few important elements that account for most problems. It builds on the well known insight that a few reasons explain or allow to control most of the outcome. This applies particularly well in the technological and industrial context. Often in workshop setups the priorities of day are set by the informal discussions between team members. It is important to be sensitive to the last problem observed or the latest request from management but it is also important to look at data, particularly over a period of time. When looking at simple things as counts and frequencies we sometimes get surprised of how different our perception is from the reality shown by the data collected. Collecting data can be done in many different forms and there’s no right or wrong. It can be noted on a board, log book, spreadsheet or in a dedicated software. In a dial polishing workshop of a watchmaking manufacture, the assembly operators have been collecting dial defects in a spreadsheet. Logging a defect doesn’t mean the dial is directly scrapped. Putting away parts has strong impact on the cost of the operation and has to be done on clear criteria. Sometimes the parts can be rework with minor effort. The datalog corresponds to the status of the dials as they arrive from the stamping and before entering the polishing operation. Their dataset with the name dial_control shows each dial unique number and the general and defect information noted by the operators. Collecting defects All datasets are available by loading the book companion package with library(industRial). Full instructions in the datasets session. head(dial_control) %&gt;% kable(align = &quot;c&quot;, caption = &quot;dial control data&quot;, booktabs = T) Table 1: dial control data Operator Date Defect Location id Jane 2018.01.31 Indent 3h D2354 Jane 2018.02.02 Indent 3h D2355 Jane 2018.02.02 Indent 4h D2356 Peter 2018.02.02 Indent 10h D2357 Jane 2018.02.03 Scratch 3h D2358 Jane 2018.02.03 Indent 3h D2359 We can see that the count includes both the defect type and the location (the hour in the dial) and that it is traced to the day and operator. The team leader promotes a culture of fact based assessment of the quality measurements. Every week the team looks back and observes the weekly counts. When the quantity of data get bigger trends to start becoming apparent. The team can discuss potential actions and prepare reporting to the supplier of the parts (the stamping workshop). It also helps calibrating between operators and agreeing on acceptance criteria and what is and what is not a defect. Recently there have been lots of talk about scratched dials and there’s a big focus on how to get rid of them. For their weekly review Christophe has prepared a Pareto chart in R. Pareto chart See {qicharts2} for more details on this R package library(qicharts2) d_type &lt;- dial_control %&gt;% pull(Defect) %&gt;% as.character() d_type_p &lt;- paretochart(d_type, title = &quot;Watch dial Defects&quot;, subtitle = &quot;Pareto chart&quot;, ylab = &quot;Percentage of defects&quot;, xlab = &quot;Defect type&quot;, caption = &quot;Source: dial polishing workshop&quot;) d_type_p + theme_industRial() As often happens we can see that the first two defects account for more than 80% of the problems. Scratching levels are in fact high but they realize indentation is even higher. Is it clear what indentation is? Have we been noting sometimes indentation for scratches? Where to draw the line? and are the causes of these two defects the same? The decides to go deeper in the analysis and Peter says that a potential cause is the fixing tool that holds the dial on the right. To check Peter’s hypothesis Jane prepares another plot by location for the next week review. d_location &lt;- dial_control %&gt;% pull(Location) %&gt;% as.character() d_location_p &lt;- paretochart(d_location, title = &quot;Watch dial defects&quot;, subtitle = &quot;Pareto chart&quot;, ylab = &quot;Percentage of defects&quot;, xlab = &quot;Defect location (hour)&quot;, caption = &quot;Source: Dial workshop&quot;) d_location_p + theme_industRial() Effectively there are many defects at 3h corresponding to the position on the right of the dial (and even more at 4h). Peter’s assumption may be right, the team decides to gather in the first polishing workbench and share openly how each of them fixes the dial to try to understand if there is a specific procedure or applied force that creates the defect. This example shows how data collecting can be simple and effective. if no one in the team is using R yet, a simple pareto chart could be done more simply with a spreadsheet. What R brings is the possibility to quickly scale up: handling very large and constantly changing files for example and also the possibility to directly and simply produce PDF reports or dynamic web applications to collect and visualize the data. To practice and go further in the exploration of pareto charts checkout the tutorials section. Root cause analysis Case study: dental prosthesis laboratory An optical measurement device has just been installed in a large Dental Prosthesis Manufacturing Laboratory. It is precise but expensive device based on laser technology which has been installed in a dedicated stabilized workbench. Usually called Fishbone or Ishikawa diagrams this simple tool has proven to be extremely practical and helpful in structuring team discussions. With it we can easily identify and list the expected influencing factors in various contexts such as the preparation of an experiment design. Selection and grouping input parameters can be useful in defining for example the right mix of ingredients in a new product, in selecting manufacturing parameters in an industrial production line or in the definition of a draft operating procedure for a measurement device. In each of these situations it helps seeing the big picture and not fall into the trap of relying only in the data and findings obtained by statistical analysis. In this case study we’re exploring the creation of Ishikawa diagrams with the {qcc} package. Emilio L. Cano (2012) recommends the utilization of R even for such simple diagrams with clear arguments on reproducibility and ease of update. If R and programming is already part of the working culture and there’s someone in the team this makes perfect sense. The lab manager of a dental prosthesis laboratory has acquired a optical device for the precise measurement of the dental impressions that serve as models for the production of the crowns and bridges. The lab has been having complains and several parts have been returned from the dentists and had to be partially or totally reworked. Besides the potential troubles to patients and the already incurred financial losses there is a reputation loss of which the lab manager is very concerned with. Regardless of all this the acquisition decision has taken more than a year. After installation and in spite all the precautions it has been reported and now demonstrated with some specific trials that the measurements have a high variation which is preventing putting it in operation. Until now the laboratory team has always had full confidence in the equipment supplier and the Lab Manager has even seen the same equipment operating in another laboratory from the group. The supplier has been called on site to check the equipment and having seen no reason for the variability proposes to work with the lab team on identifying the potential causes for the high uncertainty in their measurements. They decided to consider a larger scope than just the equipment and take the full measurement method as described in the laboratory operating procedure. They organize a brainstorm, list different reasons related with they’re work and group them. Brainstorming operators &lt;- c(&quot;Supplier&quot;, &quot;Lab Technician&quot;, &quot;Lab Manager&quot;) materials &lt;- c(&quot;Silicon&quot;, &quot;Alginate&quot;, &quot;Polyethers&quot;) machines &lt;- c(&quot;Brightness&quot;, &quot;Fixture&quot;, &quot;Dimensional algorithm&quot;) methods &lt;- c(&quot;Fixture&quot;, &quot;Holding time&quot;, &quot;Resolution&quot;) measurements &lt;- c(&quot;Recording method&quot;, &quot;Rounding&quot;, &quot;Log&quot;) groups &lt;- c(&quot;Operator&quot;, &quot;Material&quot;, &quot;Machine&quot;, &quot;Method&quot;, &quot;Measurement&quot;) effect &lt;- &quot;Too high uncertainty&quot; One of the team members is using R and he has generating all previous reports on the topic with R markdown. He simply adds to the last report a call to the {qcc} package and quickly obtains a simple diagram that allows for a quick visualization of these influencing factors. Ishikawa digram library(qcc) cause.and.effect( title = &quot;Potential causes for optical measurement variation&quot;, cause = list( Operator = operators, Material = materials, Machine = machines, Method = methods, Measurement = measurements ), effect = effect ) The listed factors can now be addressed either one by one or in combined experiments to evaluate their impact on the measurement method. The lab team has decided to assess the method robustness to the brightness and to the dimensional algorithm and will prepare an experiment design with several combinations of these parameters to evaluate them. Using the diagram they can easily keep track of what has been listed, tested and can be eliminated as root cause. Correlations Case study: perfume distillation experiment A Project Manager in perfume formulation needs to understand in detail the impact of the perfume manufacturing line parameters variation (e.g. temperature, pressure and others) in typical perfume sensorial characteristics such as the floral notes. A correlation matrix is a way to discover relationships between groups of items. Such matrix can also be used to select which output measurement should be done in priority in a design of experiments (DOE). In exploratory phases when the experiments are repeated several time with slightly different configurations, secondary outputs that are strongly correlated to main outputs can be eliminated In an industrial setup the cost of experimenting is often very high. With this approach engineers and scientists can keep the test quantities in control and avoiding measurements until final stages of implementation. We explore in this case study two different techniques, one with a tile plot and another more advanced with a network plot. A DOE consists in a series of trials where several inputs are combined together and important outputs are measured (further details can be seen in the DOE chapter). Commonly DOE analysis results linking inputs to outputs are presented with effects plots and interaction plots but before getting it is important to check the correlation between the outputs. Often there groups of outputs move together even if there is no cause and effect relationship between them. We can see this correlation in a tile plot. A team of experts of a manufacturer of fragrances has listed 23 different output variables of interest for an exploratory perfume distillation experiment. Facing such extensive list the Project Manager decided to put the team together a second time to try to set priorities. The approach was to guess the results of the experiment which allowed to go deeper in the technology and to construct an experiment plan in a meaningful way. The experts inputs have been captured in a a two entry table named perfume_experiment. Matrix perfume_experiment[1:6, 1:7]%&gt;% kable( align = &quot;c&quot;, caption = &quot;perfume DoE correlation matrix of the outputs&quot;, booktabs = T ) Table 2: perfume DoE correlation matrix of the outputs yy pw w pm pe f it pw 0 10 3 3 2 2 w 0 0 3 3 2 2 pm 0 0 0 6 6 0 pe 0 0 0 0 6 0 f 0 0 0 0 0 7 it 0 0 0 0 0 0 In the matrix the experiment output variables are named with coded names made of two letters. They represent the production Line Parameters (e.g. t = temperature, o = opening, pw = power) and the Perfume Attributes (f = flower). We can see in the table what the experts have noted the expected correlation strengths in an unusual way from 1 to 10, with 10 being the highest. In order to prepare a visual representation with a tile plot from {ggplot2} the data is transformed to long format. An additional trick is to convert the values at zero to NA so that they get directly transparent on the plot. Values at zero in the dataset are converted to type NA_real_ to obtain a transparent background in the the tileplot. perfume_long &lt;- perfume_experiment %&gt;% pivot_longer( cols = -yy, values_to = &quot;correlation&quot;, names_to = &quot;xx&quot; ) %&gt;% mutate(correlation = if_else( correlation == 0, NA_real_, correlation)) %&gt;% mutate(correlation = as_factor(correlation)) Tileplot perfume_long %&gt;% ggplot(aes(x = xx, y = yy, fill = correlation)) + scale_fill_viridis_d(direction = -1, name = &quot;Correlation\\nStrength&quot;) + geom_tile() + labs( title = &quot;The Perfume distillation experiment&quot;, subtitle = &quot;Output variables correlation plot &quot;, x = &quot;&quot;, y = &quot;&quot;, caption = &quot;Anonymous data&quot; ) + theme_industRial() The plot shows that many parameters are expected to move together. Looking in detail the flow aroma moves together with other sensory attributes such as hp, o and oc. After this first DoE the real correlations will be established and the team expects to be able to avoid a significant part of the measurements that have a correlation higher than 50% from the second DoE onward. Clustering In this second analysis of the perfume distillation experiment we present a more advanced but more powerful approach using network plots. It explores an automatic way to clustering the variables and a specific way to present such clusters. Technically we’re going to build a weighed non directional network(tbl_graph) object. Several steps of conversion are required for this approach first with functions from various packages from the networks domain. library(igraph) library(tidygraph) library(ggraph) The first step consists in converting the “Perfume” tibble to a matrix format: The perfume_experiment is originally coded as a tibble object. perfume_matrix &lt;- perfume_experiment %&gt;% column_to_rownames(&quot;yy&quot;) %&gt;% as.matrix() Then using the {igraph} package we convert the matrix into a graph object: perfume_graph &lt;- graph_from_adjacency_matrix( perfume_matrix, mode = &quot;undirected&quot;, weighted = TRUE ) to finally convert it into a tibble graph with {tidygraph} package: perfum_tbl_graph &lt;- as_tbl_graph(perfume_graph, add.rownames = &quot;nodes_names&quot;) As mentioned the experts have provided the correlation strength in the unusual scale from 1 to 10 which was easier for them during discussion. Here we’re here converting it back to the 0 to 1 which is more common in the statistics community. For simplicity, negative correlations were not considered just the strength, enabling the network to be unidirectional. perfum_tbl_graph &lt;- perfum_tbl_graph %&gt;% activate(edges) %&gt;% mutate(weight = weight/10) perfum_tbl_graph # A tbl_graph: 22 nodes and 85 edges # # An undirected simple graph with 7 components # # Edge Data: 85 x 3 (active) from to weight &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 2 1 2 1 3 0.3 3 1 4 0.3 4 1 5 0.2 5 1 6 0.2 6 1 8 0.8 # … with 79 more rows # # Node Data: 22 x 1 name &lt;chr&gt; 1 pw 2 w 3 pm # … with 19 more rows In the previous chunk output we see a preview of the tibble graph object with the first few nodes and edges. Now we create a vector with various igraph layouts to allow for easier selection when making the plots: igraph_layouts &lt;- c(&#39;star&#39;, &#39;circle&#39;, &#39;gem&#39;, &#39;dh&#39;, &#39;graphopt&#39;, &#39;grid&#39;, &#39;mds&#39;, &#39;randomly&#39;, &#39;fr&#39;, &#39;kk&#39;, &#39;drl&#39;, &#39;lgl&#39;) and do a first network plot to check data upload: perfum_tbl_graph %&gt;% ggraph::ggraph(layout = &quot;igraph&quot;, algorithm = igraph_layouts[7]) + geom_edge_link(aes(edge_alpha = weight)) + geom_node_label(aes(label = name), repel = TRUE) + # theme_graph() + labs(title = &quot;DOE Perfume Formulation - Inputs&quot;, subtitle = &quot;Most important expected correlations&quot;) Data loading is now confirmed to have been done correctly and we can now move into the clustering analysis. We use different clusters algorithms to generate the groups. Clustering algorithms perfum_tbl_graph &lt;- perfum_tbl_graph %&gt;% activate(nodes) %&gt;% mutate(group_components = group_components(), group_edge_betweenness = group_edge_betweenness(), group_fast_greedy = group_fast_greedy(), group_infomap = group_infomap(), group_label_prop = group_label_prop(), group_leading_eigen = group_leading_eigen(), group_louvain = group_louvain(), group_walktrap = group_walktrap() ) There’s extensive research behind of each of these algorithms and detailed information can be obtained starting simply with the R help system. For example for one selected here type ?group_louvain or ?cluster_louvain on the console. Digging deeper it is possible to find the author names and the papers explaining how and when to use them. To produce the final plot some trial and error is needed to select the algorithm that gives the best clustering results. Now for the final step we also need to load some specific support packages for advanced plotting. library(ggforce) library(ggtext) Network plot perfum_tg_2 &lt;- perfum_tbl_graph %&gt;% activate(edges) %&gt;% mutate(weight2 = if_else(weight &gt;= 0.8, 1, if_else(weight &gt;= 0.5, 0.5, 0.1))) my_palette &lt;- c(viridis(12)[3], viridis(12)[9], &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;, &quot;gray40&quot;) set.seed(48) perfum_tg_2 %&gt;% activate(nodes) %&gt;% mutate(group = group_louvain) %&gt;% filter(group %in% c(1,2)) %&gt;% ggraph(layout = &quot;igraph&quot;, algorithm = igraph_layouts[7]) + geom_mark_hull(mapping = aes(x, y, group = as_factor(group), fill = as_factor(group)), concavity = 0.5, expand = unit(4, &#39;mm&#39;), alpha = 0.25, colour = &#39;white&#39;, show.legend = FALSE) + geom_edge_link(aes(edge_alpha = weight2, edge_width = weight2)) + geom_node_point(size = 3) + geom_node_label(aes(label = name), repel = TRUE) + scale_edge_width(range = c(0.2, 1), name = &quot;Correlation strength&quot;) + scale_edge_alpha(range = c(0.05, 0.2), name = &quot;Correlation strength&quot;) + scale_fill_manual(values = my_palette) + # theme_graph() + labs( title = str_c(&quot;&lt;span style=&#39;color:#433E85FF&#39;&gt;Line Parameters&lt;/span&gt;&quot;, &quot; and &quot;, &quot;&lt;span style=&#39;color:#51C56AFF&#39;&gt;Perfume Attributes&lt;/span&gt;&quot;), subtitle = &quot;Clustering the outputs of Perfume Formulation DOE01&quot;, caption = &quot;Clustering by multi-level modularity optimisation (louvain)&quot;) + theme(plot.title = element_markdown(family = &quot;Helvetica&quot;, size = 14, face = &quot;bold&quot;)) We can see that the algorithm is grouping elements that have a strong correlation. Most stronger correlations are mostly presented within elements of each cluster. This is expected as certain perfume sensorial attributes are strongly correlated and the same for certain Line Parameters. The code presented can now be reused once the DOE is executed to compare with the real correlations measured. Once knowledge is built and confirmed on which outputs are strongly correlated a selection of the key parameters can be done. This simplifies the experiments by reducing the number of outputs to measure and reduces the cost and lead time of new formulations. References "],["MSA.html", "Measurement System Analysis Calibration Precision Uncertainty", " Measurement System Analysis Analyzing and validating measurement methods and tools is the base for ensuring the quality of manufacturing products. For most commercial products it is not simply about satisfying consumer expectations but has regulatory and legal implications. Using measurement tools in industrial setups for high volume production goes naturally beyond buying and installing an equipment. It requires clear operating procedures, trained operators and tested devices for the specific range applications and products. There are many different normalizing bodies in the metrology domain with different approaches and terminology. The cases in this section follow a simplified step by step approach aiming at giving an overview of how data treatment can be done with R. The first case treats the calibration of a recently acquired measurement device by comparing it to a reference device. It provides statistical analysis of the bias of the method compared with the reference for the full measurement range. The following case deals with the estimation of the method precision, namely the measurement repeatability and reproducibility under regular utilization conditions. It provides examples on acceptance criteria typical in industrial context. The final case study presents calculation of the method uncertainty, a more comprehensive indicator taking into account the calculations done in the previous cases. Calibration Case study: juice production plant The Quality Assurance Head has acquired a fast dry matter content measurement device from the supplier DRX. The rational for the acquisition has been the important reduction of the control time. Before it enters operation its performance is being assessed and validated. A first step after a measurement equipment acquisition is the assessment of the response over the entire measurement range. In particular it is important to verify its linearity and variability and determine the average bias throughout the measurement range. In a juice production plant the dry matter content for the top seller is around 13% dry matter content. Typical specifications are the Premium fresh apple juice with 12.4 % and the Austrian beetroot juice with 13.2%. Some other specialties may have a higher content up such as the Organic carrot that has 16.3%. After consulting with the Manufacturing Team Leader, the Quality Assurance Head selects checking the equipment in the range of 10 to 20% dry matter content. For the calibration assessment samples are produced at target values set at round numbers (10%, 15% and so on). This data is captured in the juice_drymatter dataset of which we’re checking juice_drymatter %&gt;% head(5) %&gt;% kable( align = &quot;c&quot;, caption = &quot;juice dry matter data&quot; ) Table 3: juice dry matter data product drymatter_TGT speed particle_size part drymatter_DRX drymatter_REF apple 10 20 250 1 9.80 10.05 apple 10 20 250 2 9.82 10.05 apple 10 20 250 3 9.82 10.05 beetroot 10 20 250 1 9.79 10.03 beetroot 10 20 250 2 9.75 10.03 We see in this raw dataset that it contains the same samples dry matter content measured twice. First with the with the new equipment (DRX) and then with the reference equipment (Ref). The reference equipment is considered as such because it has been validated and accepted by the head quarters quality department. The difference between the two devices for each measurement is calculated below and allocated to a new variable with the name bias. juice_drymatter &lt;- juice_drymatter %&gt;% mutate(bias = drymatter_DRX - drymatter_REF, part = as_factor(part)) A first look at the bias with the skim() function from {skimr} gives already an indication that the bias is not constant along the measurement range. See {skimr} for more details on this R package, an alternative to base::summary() library(skimr) skim(juice_drymatter$bias) %&gt;% yank(&quot;numeric&quot;) Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist data 0 1 -0.3 0.14 -0.63 -0.4 -0.29 -0.19 -0.07 ▂▅▆▇▇ Such results are not encouraging because a non regular bias along the range may require specific correction for different product which may be not practical and prone to error. Often this requires to dig into detail to understand the causes of the bias and determine if they are related with the physical phenomena and if there are clear controllable causes. Ultimately this could result is narrowing the measurement range and validating a specific device and method for a specific product specification target. For the Quality Assurance Manager it is too early to draw conclusions and he establishes a more detailed plot with {ggplot2} to better visualize the data. Bias plot juice_drymatter %&gt;% ggplot(aes(x = drymatter_REF, y = bias)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = T, ) + coord_cartesian( xlim = c(9,21), ylim = c(-.75,0), expand = TRUE) + theme_industRial() + labs(title = &quot;Dry matter method validation&quot;, subtitle = &quot;Gage Linearity&quot;, caption = &quot;Dataset: juice_drymatter233A, Operator: S.Jonathan)&quot;) This type of plot is usually called bias plot and provides a view of how the difference between the measurements obtained with the new device and the reference device compare along the measurement range. In the plot generated an additional regression line has been introduced with geom_smooth from {ggplot2}. There are several ways to assess the linearity. In this case we’re going to remain at a visual check only leaving to the Design of Experiments case study a more thorough verification. The linear model appears as well adapted in this case. The first check is the observation that regression line passes close to the averages of each level of the dry matter factor. Nevertheless the slope is rather steep showing a clear increase of the bias (in the negative direction) with the increase in dry matter content. Bias report Using well known {dplyr} function the plot is complemented with statistics of the bias for each level of dry matter target: mean, median, standard deviation. A good practice that took some time to adopt but now is well anchored is to always present the sample size which speaks for the relevance of the statistical indicators. juice_drymatter_bias &lt;- juice_drymatter %&gt;% group_by(drymatter_TGT) %&gt;% summarise(bias_mean = mean(bias, na.rm = TRUE), bias_median = median(bias, na.rm = TRUE), bias_sd = sd(bias, na.rm = TRUE), bias_n = n()) juice_drymatter_bias %&gt;% kable(align = &quot;c&quot;, digits = 2) drymatter_TGT bias_mean bias_median bias_sd bias_n 10 -0.17 -0.15 0.07 36 15 -0.29 -0.31 0.10 36 20 -0.44 -0.44 0.10 36 Mean and median bias are very close which indicates that the data is equally distributed around the mean The standard deviation is also very similar from level to level indicating that the measurement variability is not depending on the range of measurement. A decision now needs to be taken on which systematic offset to apply depending on the operational context. As mentioned most commercial products on the production line where the device is used have a target specification around 13% therefore the Quality Assurance Head decides together with Manufacturing Team Leader to put in the operating procedure of the device a unique offset of 0.3 g. This value is assigned to a new variable called juice_cal_u that will be needed later to calculate the measurement uncertainty. u_cal &lt;- as_vector(juice_drymatter_bias[2,4]) names(u_cal) &lt;- NULL # we&#39;re removing the original name to avoid confusion later. Precision Case study: tablet compaction process Modern pharmaceutical tablet presses reach output volumes of up to 1,700,000 tablets per hour. These huge volumes require frequent in-process quality control for the tablet weight, thickness and hardness. Pharmaceutical production setups combine extreme high volumes with stringent quality demands. In this context many manufacturing plants have inline automatic measurement devices providing automatic data collection to a central database but it is not uncommon to see hand held devices and manual log of measurements in spreadsheets. In an age of machine learning and sophisticated predictive tools this may seem awkward but it is common to see coexisting old and new approaches on the shop floor. A recurring check of measurement devices is the famous gage r&amp;R. r&amp;R stands for reproducibility and Reproductibility which combined give the instrument precision, according to the ISO 5725. In any case automatic or manual the way to assess the measurement device should follow the same approach. In our case we’re looking into a pharmaceutical company where a tablet compaction process the quality measurement system requires the Production Operator to sample tablets on a regular basis and log the thickness in a spreadsheet on the line. Measurements are done with a micrometer build and acquired specifically for this purpose that has a fixture developed to fit the shape of the tablet. Besides thickness, the quality measurement system requires the operator to collect quite an large variety of parameters including room conditions too. Elaborating on this a Quality Engineer has prepared a specific file for the gage r&amp;R that also included the replicate number. As it is common practice he asked the measurements to be done by several operators. This data has been loaded into R and is available in the dataset tablet_thickness and an extract is presented here in raw: tablet_thickness %&gt;% head(3) %&gt;% kable( align = &quot;c&quot;, caption = &quot;tablet thickness gage r&amp;R data&quot; ) Table 4: tablet thickness gage r&amp;R data Position Size Tablet Replicate Day Date [DD.MM.YYYY] Operator Thickness [micron] Temperature [°C] Relative Humidity [%] Luminescence [lux] Position 1 L L001 1 Day 1 18/11/2020 Paulo 1802.5 22.3 32.7 568.6 Position 1 L L001 2 Day 1 18/11/2020 Paulo 1802.8 22.3 32.8 580.4 Position 1 L L001 3 Day 1 18/11/2020 Paulo 1804.0 22.3 32.8 580.5 It is an excellent practice to look at raw data because it gives an immediate perception of general aspects such as the number of variables, their levels and their datatypes. Although this is irreplaceable it is possible to go further and skim() provides an excellent complement and summary. Below we see that the test requested by the Quality Engineer has required 675 measurements on 11 different variables by 3 different operators. We can see room conditions are stable, rather normally distributed and having small standard deviations and we can even see that thickness appears with 3 groups which seems related with the 3 sizes noted in the Size column. skim(tablet_thickness) The initial idea of the Quality Engineer was to establish a separate gage r&amp;R by tablet size. There is sometimes debate if in the study several different specification should be combined or not. In the last quality weekly meeting this was reason for lively discussions with various logical arguments from the Production Leader and the Engineering Manager. They ended up accepting the proposal of a separate gage per size on the logic that it is important to compare the measurement method variability not only with the process variability but also with the specification itself. Data in excel files to have most of the time human readable formats and the files being open they usually end up with long variable names. Unlike the classical read.csv() function from the base R the read_csv() function from {readr} is not converting character variables to factors. This is a good behavior in our view because it allows for better control and awareness of what is happening. In this case the Quality Engineer is acquainted to the {tidyverse} and is now making the conversion specifically on the desired variables size, tablet and operator. He also makes the filtering for the size L for which he will do the first r&amp;R study. tablet_thickness &lt;- tablet_thickness %&gt;% clean_names() %&gt;% mutate(across(c(size, tablet, operator), as_factor)) tablet_L &lt;- tablet_thickness %&gt;% filter(size == &quot;L&quot;) Now that the dataset is clean and ready he moves forward with the ss.rr() function from the {SixSigma} package. Gage r&amp;R library(SixSigma) # dimensions for chunk output when included: fig.dim=c(8, 10) tablet_L_rr &lt;- ss.rr( data = tablet_L, var = thickness_micron, part = tablet, appr = operator, alphaLim = 1, errorTerm = &quot;repeatability&quot;, main = &quot;Micrometer FTR600\\nr&amp;R for tablet thickness&quot;, sub = &quot;Tablet L&quot;, lsl = 1775, usl = 1825 ) The ss.rr function takes the filtered tablet_L dataset and var, part and the arguments appr to precise what is the measurement variable, the part and the operator in this order. Then to be noted that the alphaLim argument is set to 1 in this first assessment. This is to keep all the model terms including non significant one. In future analysis this can be set to 0.05 the usual significance threshold and those non significant terms are omitted. Another detail important to ensure is to select the repeatability as the errorTerm otherwise we get different results than those obtained with base anova and other software aligned with the Automotive Industry Action Group (2010) guidelines such as Minitab. Finally the function also allows to input the limits he also provides in the arguments the current upper and lower limit of the specification, in this case of 1’800 \\(\\mu m\\) +/- 25 \\(\\mu m\\) for tablet L. The output of this function is a list with several elements inside and an automatically generated report. names(tablet_L_rr) [1] &quot;anovaTable&quot; &quot;anovaRed&quot; &quot;varComp&quot; &quot;studyVar&quot; &quot;ncat&quot; We’re now looking more in detail in some of them. Gage acceptance Measurement system acceptance can be done based on varied criteria and is often done in progressive stages. In Research and Development contexts it is common that the measurement method is developed simultaneously with the end product. There are stages where the teams are conceiving the full industrial setup and there may be an overlap between product sub-assembly, assembly machine and measurement device. These different components of the production or assembly line may not reach maturity all at the same time. In such cases the Quality Assurance may give an approval for the measurement device based on tests done on products that cannot yet be commercialized. This means that the final conditions of usage are not fully tested. In other cases the measurement method is complex but time presses and the teams test the quality of the parts by other means such as the failure rates of the assemblies where the parts go. For all these reasons it is important to clarify at all times the assumptions used in the assessment of the measurement method. Variance components A common way to quickly judge if an equipment variability is high is to look at its variance. In our case the Quality Engineer can look at the variance components of the gage r&amp;R study by calling them from the ss.rr list. tablet_L_rr$varComp %&gt;% kable(digits = 1) VarComp %Contrib Total Gage R&amp;R 1.6 14.8 Repeatability 1.6 14.2 Reproducibility 0.1 0.6 operator 0.1 0.6 tablet:operator 0.0 0.0 Part-To-Part 9.5 85.2 Total Variation 11.1 100.0 Looking at the column %Contrib he sees that the total gage R&amp;R is too high when comparing with the established guidelines for gage acceptance: Less than 1%: the measurement system is acceptable Between 1% and 9%: the measurement system is acceptable depending on the application, the cost of the measurement device, cost of repair, or other factors Greater than 9%: the measurement system is not acceptable and should be improved. Another direct information from this assessment is that this variability comes from the repeatability mostly and not from the operator or from the interaction. This is very useful as a clue to start identifying where the variability comes from and how to try to improve it. Although quick and providing a first impression, variance is not a very intuitive statistic as is not expressed in the measurement units. A much more common and speaking approach is to look into the standard deviation and compare it with the process variation but also with the specification itself. Standard deviation components The standard deviation values from the study can be pulled from the list with the same approach as before. tablet_L_rr$studyVar %&gt;% kable(digits = 1) StdDev StudyVar %StudyVar %Tolerance Total Gage R&amp;R 1.3 7.7 38.5 15.4 Repeatability 1.3 7.5 37.6 15.1 Reproducibility 0.3 1.6 7.9 3.1 operator 0.3 1.6 7.9 3.1 tablet:operator 0.0 0.0 0.0 0.0 Part-To-Part 3.1 18.4 92.3 36.9 Total Variation 3.3 20.0 100.0 40.0 The study variation table is has several columns. The StdDev column contains the square root of each individual variance. The StudyVar column has each StdDev multiplied by 6 which corresponds to the max variability for each component. Then each StudyVar is divided by the Total Variation and expressed in percentage in the %StudyVar column. The last column %Tolerance contains the division of the StudyVar by the specification interval (+/- 25 \\(\\mu m\\) in this case) expressed in percentage. The Quality Engineer is now is a position to progress is assessment. According to the guidelines followed in the company the measurement method variation needs to be less than 10% of the process variation to be considered directly accepted. This is expressed here in the column %StudyVar and is 38.46% which is much above this limit. The guidelines state: Less than 10%: the measurement system is acceptable Between 10% and 30%: the measurement system is acceptable depending on the application, the cost of the measurement device, cost of repair, or other factors Greater that 30%: the measurement system is not acceptable and should be improved. As he already knew, the variability is mostly coming from the repeatability. With this approach he can also compare with the product specification tolerance which is 15.37%. The part to part variation corresponds to the bulk of the variability and this is what is expected. Although the Quality Assurance department is not fully validating a measurement method with these figures there seems to be potential to improve the situation. At this moment we can consider that the micrometer allows to sort good parts from bad because the variability is lower than 30% of specification tolerance but it cannot be used to drive production as the variation is higher than 30% of the production process variation. Gage plots Besides the Variance the Standard Deviation components the ss.rr function generates a full report. We’re going to look into at each of the plots by generating them with the custom function ss.rr.plots. The details of the function itself are presented afterwards. tablet_L_rr_plots &lt;- ss.rr.plots( data = tablet_L, var = thickness_micron, part = tablet, appr = operator, alphaLim = 1, errorTerm = &quot;repeatability&quot;, main = &quot;Micrometer FTR600\\nr&amp;R for tablet thickness&quot;, sub = &quot;Tablet L&quot;, lsl = 1775, usl = 1825 ) plot(tablet_L_rr_plots$plot1) This first bar plot presents in a graphical way the gage results in percentage and we can quickly grasp if we’re on target by looking at the pink bars and comparing them with the dashed bars. We can see that the G.R&amp;R is above 30% and thus is not acceptable from a Study Variation criteria. In green we see that compared with the specification we’re above 10% but below 30% so acceptable but requiring improvement. We’re now looking into the measurements themselves: plot(tablet_L_rr_plots$plot6) plot(tablet_L_rr_plots$plot5) The previous two plots show the measurements for each operator. The first is the Ranges plot showing the differences between the min and max measurement for each part and the second plot is the means plot showing the mean thickness for each part by operator. These help showing that there is a consistency between operators and help as a diagnosis tool to identify if there would be strange patterns appearing where one of the operators would be for instance systematically measuring very high values. The next two plots show average values by part with all operators combined: plot(tablet_L_rr_plots$plot2) plot(tablet_L_rr_plots$plot3) We quickly see the measurements tend to be symmetrically distributed around their means and that the means between the different operators are very similar. This confirms the low reproducibility what has been seen in the Variance Components. plot(tablet_L_rr_plots$plot4) This final plot is the so called interaction plot and if there were diverting and strongly crossed lines would indicate that different operators measure the parts in different ways. Again here this confirms the low value obtained for the interaction in the Variance Components table. Tablet thickness measurements obtained with a gage r&amp;R study done on a pharmaceutical tablet compaction process. Red dashed lines corresponds to the thickness specification limits. When the gage report was shared with the Production Leader and the Engineering Manager they raised concerns regarding which is how big is our process variability and how much is the process centered. These are valid points as we often go back and forth between measurement validation, product development and process control. Measurement validation makes us look into details on the measured values and question their validity. Taking conscience that the production specification is not adapted, too large or too narrow. Often we realise that production is systematically slightly off centered. Depending on the diagnostic a new gage r&amp;R plan and sampling may need to be prepared and the process or the specification adjusted. Such topics will be further discussed in the Design of Experiments and in the Statistical Process Control subjects. Negative variance We’ve started the gage assessment by setting the errorTerm to 1. This made that factors that were non significant remained visible, in our case this happened with the tablet:operator interaction. Although the ss.rr function is always showing zero for non significant factors it may happen that in reality the calculated value is negative. We refer to page 557 Montgomery (2012) to get guidance on how to address this case: note that the P-value for the interaction term […] is very large, take this as evidence that it really is zero and that there is no interaction effect, and then fit a reduced model of the form that does not include the interaction term. This is a relatively easy approach and one that often works nearly as well as more sophisticated methods. This approach is the one implemented in {SixSigma}. When we leave the argument alphaLim empty the non significant terms are be suppressed from the model, the Anova is recalculated and the remaining tables updated accordingly. We can control this behavior by playing with different values. Usually we consider a p value of 0.05 but we recommend to start with higher values such as 0.1 or 0.2 to avoid suppressing too quickly the factor which would result in a transfer of their variability into the repeatability. Below we run again the ss.rr function with a limit at 0.05 and get the entire data and plot output in one single step. In our case adjusting the p value has had a very limited impact in the total gage r&amp;R which has changed only from 38.46% to 38.38%. tablet_L_rr2 &lt;- ss.rr( data = tablet_L, var = thickness_micron, part = tablet, appr = operator, alphaLim = 0.05, errorTerm = &quot;repeatability&quot;, main = &quot;Micrometer FTR600\\nr&amp;R for tablet thickness&quot;, sub = &quot;Tablet L&quot;, lsl = 1775, usl = 1825 ) Complete model (with interaction): Df Sum Sq Mean Sq F value Pr(&gt;F) tablet 4 1707 427 271.46 &lt;2e-16 operator 2 13 7 4.18 0.017 tablet:operator 8 11 1 0.89 0.524 Repeatability 210 330 2 Total 224 2062 alpha for removing interaction: 0.05 Reduced model (without interaction): Df Sum Sq Mean Sq F value Pr(&gt;F) tablet 4 1707 427 272.53 &lt;2e-16 operator 2 13 7 4.19 0.016 Repeatability 218 341 2 Total 224 2062 Gage R&amp;R VarComp %Contrib Total Gage R&amp;R 1.632605 14.73 Repeatability 1.565919 14.13 Reproducibility 0.066685 0.60 operator 0.066685 0.60 Part-To-Part 9.448857 85.27 Total Variation 11.081462 100.00 VarComp StdDev StudyVar %StudyVar %Tolerance Total Gage R&amp;R 1.632605 1.27773 7.6664 38.38 15.33 Repeatability 1.565919 1.25137 7.5082 37.59 15.02 Reproducibility 0.066685 0.25823 1.5494 7.76 3.10 operator 0.066685 0.25823 1.5494 7.76 3.10 Part-To-Part 9.448857 3.07390 18.4434 92.34 36.89 Total Variation 11.081462 3.32888 19.9733 100.00 39.95 Number of Distinct Categories = 3 Further developments on the gage r&amp;R in the excellent book from Springer by the {SixSigma} package author Emilio L. Cano (2012). Custom functions The original report generated by the ss.rrfunction has the inconvenient of being generated as a single plot. Another inconvenient is that there is no option in the function to suppress it in case we just want to look at the data output. To obtain the individual plots presented in this unit the original function code has had to be modified. This possibility to reuse and modify the code from other authors is one of the great benefits of R. This is possible because packages are is distributed under a license from the Free Software Foundation. Licenses are long are difficult to read but by simply typing RShowDoc(\"GPL-3\") we can already read in the first few lines you can change the software or use pieces of it in new free programs. The {SixSigma} package itself is also under the same license: SixSigmaDescription &lt;- utils::packageDescription(&quot;SixSigma&quot;) SixSigmaDescription$License [1] &quot;GPL (&gt;= 2)&quot; Now that this is all clarified the ss.rr function code can the be obtained in RStudio by selecting the package environment in the environment pane and looking for the function. A more direct approach is by simply typing ss.rr on the console. The full code is then revealed and can be copied and modified. For the {industRial} package we’ve copied the code in a new function which we called ss.rr.plots that generates as output a list of plots. Each plot can now be plotted individually. Uncertainty In the Pharmaceutical company described in this case study, the final formal Measurement System Analysis reports are issued with a statement on uncertainty. This is a way to combine this various intermediate assessments described before and to communicate the result in a format that can be interpreted by the persons who read measurement results such as Product Development scientists and the R&amp;D management. Different companies adopt more or less sophisticated norms which provide a detailed way of calculating the combined uncertainty that comes from the different assessments performed. In this case study we’re presenting a simple summation in quadrature equivalent to the one described by Bell (2001) page 14: \\[ u=\\sqrt{u_{man.}^2 + u_{cal.}^2 + u_{repeat.}^2+ u_{reprod.}^2} \\] This formula consists in taking the square root of the sum of the squares of the standard deviations obtained in the different tests. The first term is coming from the micrometer manufacturer which mentions in the equipment guide an accuracy of 0.001 mm which corresponds to 1 \\(\\mu\\). We assign this in R to the u_man variable: u_man &lt;- 1 u_man [1] 1 The calibration uncertainty has been established before in the calibration study : u_cal [1] 0.10244 The repeatability and reproducibility uncertainties correspond to the standard deviations calculated in the r&amp;R study. In our case we can even obtain them directly from the Variance Components table generated by the ss.rr function of the {SixSigma} package that has bee discussed in details the Gage acceptance unit. We are going to name \\(u_{repeat}^2\\) as u_repeat \\(u_{reprod}^2\\) as u_reprod getting: u_repeat &lt;- tablet_L_rr$studyVar[2,1] u_repeat [1] 1.2538 u_reprod &lt;- tablet_L_rr$studyVar[3,1] u_reprod [1] 0.26241 Now putting it all together in the uncertainty formula we have: u &lt;- sqrt(u_man^2 + u_cal^2 + u_repeat^2 + u_reprod^2) u [1] 1.6283 Finally what is usually reported is the expanded uncertainty corresponding to 2 standard deviations. To be recalled that \\(\\pm\\) 2 std corresponds to 95% of the values when a repetitive measurement is made. In this case we have \\(U = 2*u\\): U &lt;- 2 * u U [1] 3.2567 For a specific measurement of say 1’800 \\(\\mu m\\) we then say: the tablet thickness is 1’800 \\(\\mu m\\) \\(\\pm\\) 3.3 \\(\\mu m\\), at the 95 percent confidence level. Written in short is: 1’800 \\(\\mu m\\) \\(\\pm\\) 3.3 \\(\\mu m\\), at a level of confidence of 95% Knowing that the specification is [1’775; 1’825] \\(\\mu\\)m we have a specification range of 50. The expanded uncertainty corresponds to 13.03 %. This is another way of looking into the ratio between method variation and specification. The {SixSigma} package gave a similar result of 15.37%. To be noted that the calculation in by the package corresponds to 3 standard deviations and does not comprise the supplier calibration. References "],["DOE.html", "Design of Experiments Direct comparisons Statistical modeling Effects significance Interactions Covariance General designs Two level designs", " Design of Experiments Companies manufacturing goods in industrial quantities have a permanent need to improve the features of their products. This is visible in any such industry be it car parts, watches, electronic components for cell phones, chocolates, clothing, medical devices, medicine, … the list could go on forever. As consumers we expect flawless quality at affordable price and we want to remain free to choose another brand if our confidence has been damaged due to a defective product. Adding to this fortunately the last decades have seen an increasing pressure to develop sustainable products that are responsibly sourced, meet stringent regulations and can last for many years and be properly disposable. Another constraint that can be observed in Research and Development is the growing awareness of the public on ethical issues. There is an increasing expectation that trials generate minimal waste and are done in a way respectful of test subjects human and animal. Experiment design provides ways to meet these important requirements by making us think upfront on what is required and how to best approach a test. Integrated in a complete solid design for quality approach it can provide deep insights on the principles of a system and support decision making based on data. A well prepared test plan minimizes trial and error and reduces the number of prototypes, measurements and time required. There are many well tested approaches, the domain is very large and our textbook can only cover a subset of the many types of DoEs used in the industry. For all these cases statistical notions are key to have a minimal preparation of the test and a valid interpretation of the results. Some statistical concepts every engineer, technician or scientist has to understand go around sampling, sample size, probability, correlation and variability. It is important to be clear about the vocabulary and the mathematics that are behind the constantly used statistics such as the mean, median, variance, standard deviation and so on. We provide a glossary and good bibliography that can be both a good starting point or a refresher. In particular the text and the case studies follow what we consider to be the most important book in this the domain, the Design and Analysis of Experiments by Montgomery (2012). Direct comparisons Case study: winter sports clothing All winter sports clothing are virtually made with a mix of natural fibers and synthetic polymers. Upgrading to recyclable polymers while keeping performance requires extensive testing of raw material characteristics such as the tensile strength. We start by exploring simple tests that compare results obtained in two samples. These cases happen all the time as everyone needs one moment or another to compare things. It can be the result of a test before and after an improvement, it can be two different materials applied in the same assembly or still different results obtained by different teams at different moments. In this case, a materials engineer working in the winter sports clothing industry is working with a polymer company to develop a textile raw material based on PET for which the mean tensile strength has to be greater than 69.0 MPa. A first delivery of samples arrives, the materials laboratory measures 28 samples and reports that the test result is not meeting the contract specification. The materials engineer is informed and get hold of the raw data, in the lab system she can see the measurement summary: summary(pet_delivery$A) Min. 1st Qu. Median Mean 3rd Qu. Max. 64.5 68.2 68.8 68.7 69.4 72.0 The mean is in fact slightly lower that the specified contract value of 69 and the materials engineer could think to confirm the rejection the batch right away. She decides nevertheless to observe how do the measurements vary. She plots the raw data on an histogram which is a very common plot showing counts for selected intervals. Histogram pet_spec &lt;- 69 pet_mean &lt;- mean(pet_delivery$A) pet_delivery %&gt;% ggplot(aes(x = A)) + geom_histogram(color = viridis(12)[4], fill = &quot;grey90&quot;) + scale_x_continuous(n.breaks = 10) + geom_vline(xintercept = pet_mean, color = &quot;darkblue&quot;, linetype = 2, size = 1) + geom_vline(xintercept = pet_spec, color = &quot;darkred&quot;, linetype = 2, show.legend = TRUE) + labs(title = &quot;PET raw material delivery&quot;, subtitle = &quot;Histogram of resistance measurements&quot;, y = &quot;Count&quot;, x = &quot;Tensile strength [MPa]&quot;, caption = &quot;Specification min in red, Batch mean in blue¨&quot;) She also observes a certain variability in the batch with many samples with measurements below specification getting close to 64 MPa. She remembers that in this case a t-test could help assessing if the mean that was obtained can be really be considered statistically different from the target value. t-test one sample t.test(x = pet_delivery$A, mu = pet_spec) One Sample t-test data: pet_delivery$A t = -1.08, df = 27, p-value = 0.29 alternative hypothesis: true mean is not equal to 69 95 percent confidence interval: 68.157 69.263 sample estimates: mean of x 68.71 The basic assumption of the test is that the mean and the reference value are identical and the alternative hypothesis is that their different. The confidence interval selected is 95% as it is common practice in the laboratory. The test result tells us that for a population average of 69, the probability of obtaining a sample with a value as extreme as 68.71 is 29% (p = 0.29). This probability value higher than the limit of 5% that she usually uses to reject the null hypothesis. In fact she cannot conclude that the sample comes from a population with a mean different than 69. She’s not sure what to do of this result and decides asking help to a colleague statistician from R&amp;D: has she applied the right? is the specification correctly defined or should it refer to the minimum sample value? Her colleague confirms that to compare means this is a good approach and as the standard deviation of the production is not available it is reasonable to use the standard deviation from the sample. This is an important detail that was not introduced explicitly as an argument in the R function. As we they are still in the initial steps of the new development they agree that it is a good idea to accept the batch. For the next deliveries the statistic recommends to try to improve the tensile strength average and reduce the variability. For the next delivery she also recommends to agree on a minimum sample size of 30 parts and to redo the t.test but for regular production the team should consider implementing a proper AQL protocol. Improving recyclability while keeping current performance is no easy task. Often novel materials are expensive as their commercial volumes are small and suppliers claim a premium on their own R&amp;D efforts. Consumers of clothing are getting more and more sensitive to waste and to recycling but they don’t always choose products with a higher price to compensate. Following the not fully successful experience with the first delivery of recyclable PET our materials engineer considers a new chemical composition that potentially increases the levels of strength. When the second delivery arrives she establishes a simple plot with the raw data to have a first grasp of the expected improvement. pet_delivery_long &lt;- pet_delivery %&gt;% pivot_longer( cols = everything(), names_to = &quot;sample&quot;, values_to = &quot;tensile_strength&quot; ) pet_delivery_long %&gt;% ggplot(aes(x = sample, y = tensile_strength)) + geom_jitter(width = 0.1, size = 0.8) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Sample&quot;, y = &quot;Tensile strength [MPa]&quot;) Choosing geom_jitter() instead of simply geom_point() avoids overlapping of the dots but has to used with caution as sometimes for precise reading can lead to mistakes. Dot plots also lack information sample statistics and a way to better understanding the bond distributions is to go for a box plot. This type of plot is somehow like the histogram seen before but more compact when several groups are required to be plotted. pet_delivery_long %&gt;% ggplot(aes(x = sample, y = tensile_strength, fill = sample)) + geom_boxplot(width = 0.3) + geom_jitter(width = 0.1, size = 0.8) + scale_fill_viridis_d(begin = 0.5, end = 0.9) + scale_y_continuous(n.breaks = 10) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Box plot&quot;, x = &quot;Sample&quot;, y = &quot;Tensile strength [MPa]&quot;) In this case she has simply added another layer to the previous plot getting both the dots and the boxes. Now she can see the median and the quantiles. The new sample has clearly higher values and she would like to confirm if the new formulation has a significant effect. While before she was comparing the sample mean with the specification, here she wants to compare the means of the two samples. A direct calculation of this difference gives: PET_meandiff &lt;- mean(pet_delivery$A) - mean(pet_delivery$B) PET_meandiff [1] -0.86286 To use the t.test it is important to have samples obtained independently and randomly, to check the normality of their distributions and the equality of their variances. To do these checks our materials engineer is using the geom_qq() function from the {ggplot} package and gets directly the normality plots for both samples in the same plot: Normality plot pet_delivery_long %&gt;% ggplot(aes(sample = tensile_strength, color = sample)) + geom_qq() + geom_qq_line() + coord_flip() + scale_color_viridis_d(begin = 0.1, end = 0.7) + labs(title = &quot;PET clothing case study&quot;, subtitle = &quot;Q-Q plot&quot;, x = &quot;Residuals&quot;, y = &quot;Tensile strength [MPa]&quot;) We observe that for both formulation the data is adhering to the straight line thus we consider that it follows a normal distribution. We also see that both lines in the qq plot have equivalent slopes indicating that the assumption of variances is a reasonable one. Visual observations are often better supported by tests such as the variance test. F-test var.test(tensile_strength ~ sample, pet_delivery_long) F test to compare two variances data: tensile_strength by sample F = 1.28, num df = 27, denom df = 27, p-value = 0.53 alternative hypothesis: true ratio of variances is not equal to 1 95 percent confidence interval: 0.59026 2.75635 sample estimates: ratio of variances 1.2755 The var.test() from the {stats} package us a simple and direct way to compare variances. The F-test is accurate only for normally distributed data. Any small deviation from normality can cause the F-test to be inaccurate, even with large samples. However, if the data conform well to the normal distribution, then the F-test is usually more powerful than Levene’s test. The test null hypothesis is that the variances are equal. Since the p value is much greater than 0.05 we cannot reject the null hypotheses meaning that we can consider them equal. Levene test library(car) leveneTest(tensile_strength ~ sample, data = pet_delivery_long) Levene&#39;s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 1 0.01 0.91 54 We had considered the samples to be normally distributed but we can be more conservative and use the leveneTest() function from the {car} package. In this case we get a p &gt; 0.05 thus again we see that there is homogeneity of the variances (they do not differ significantly). Further elaborations on the variance can be found under Minitab (2019a). The clothing sports materials engineer has now a view on the samples distribution and homogeneity of variances and can apply t.test to compare the sample means. She takes care to specify the var.equal argument as TRUE (by default it is FALSE). t-test two samples t.test( tensile_strength ~ sample, data = pet_delivery_long, var.equal = TRUE ) Two Sample t-test data: tensile_strength by sample t = -2.4, df = 54, p-value = 0.02 alternative hypothesis: true difference in means between group A and group B is not equal to 0 95 percent confidence interval: -1.58500 -0.14072 sample estimates: mean in group A mean in group B 68.710 69.573 She sees that p &lt; 0.05 and confirms the means differ significantly. The test output has also provided a confidence interval for the difference between the means at 95% probability and the mean difference calculated directly of -0.86286 falls inside this interval (to be noted that zero is obviously not included in this interval). Things look promising in the new recyclable PET formulation. Statistical modeling Case study: e-bike frame hardening Demand for electrical bicycles grows steadily and a global manufacturer is looking into improving the quality of his bicycle frames. A test program around different treatment temperatures is established to find the conditions that optimize the fatigue resistance. A way to go beyond the statistical description of samples and direct comparison between different tests it is to establish a model. Models help us simplify the reality and draw general conclusions. The case studies in this unit introduce linear models and their applications. They also serve as the backbone for statistical inference and forecasting. These are two important techniques because they provide mathematical evidence of such general conclusions in a context where the test quantities are strongly limited as for example in lifecycle testing of expensive mechanical parts. Bicycle frames are submitted to many different efforts, namely bending, compression and vibration. Obviously no one expects a bike frame to break in regular usage and it is hard to commercially claim resistance to failure as a big thing. Nevertheless on the long term a manufacturer reputation is made on performance features such as the number of cycles of effort that the frame resists. An e-bike manufacturing company is looking to increase the duration of its frames by improving the e-bike frame hardening process. A test has been run with 5 groups of 30 bike frames submitted to 4 different treatment temperature levels and the data collected in the R tibble ebike_hardening presented below: head(ebike_hardening) %&gt;% kable(align = &quot;c&quot;) temperature g1 g2 g3 g4 g5 160 575000 542000 530000 539000 570000 180 565000 593000 590000 579000 610000 200 600000 651000 610000 637000 629000 220 725000 700000 715000 685000 710000 This type of two way entry is friendly for data collection but for manipulation with the {tidyverse} package functions it is often better to transform it in a long format. ebike_narrow &lt;- ebike_hardening %&gt;% pivot_longer( cols = starts_with(&quot;g&quot;), names_to = &quot;observation&quot;, values_to = &quot;cycles&quot; ) %&gt;% group_by(temperature) %&gt;% mutate(cycles_mean = mean(cycles)) %&gt;% ungroup() slice_head(.data = ebike_narrow, n = 5) %&gt;% kable(align = &quot;c&quot;, caption = &quot;e-bike hardening experiment data&quot;) Table 5: e-bike hardening experiment data temperature observation cycles cycles_mean 160 g1 575000 551200 160 g2 542000 551200 160 g3 530000 551200 160 g4 539000 551200 160 g5 570000 551200 The engineering team is looking forward to see the first results which have been prepared by the laboratory supervisor. He has prepared a series of plots and data models and sent out an draft report. The first plot is a simple dot plot having the raw data and in red the group means. ggplot(data = ebike_narrow) + geom_point(aes(x = temperature, y = cycles)) + geom_point(aes(x = temperature, y = cycles_mean), color = &quot;red&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) Clearly the highest the furnace temperature the higher the number of cycles to failure. This is absolutely expected as higher temperatures, up to a certain level, allow to release mechanical tensions and make the material less prone to fracture. The team knows that other factors are at play such as the treatment duration, the pre-heating temperature and many others related with the welding of the frame parts, but has deliberately decided to look only into the temperature due to time constraints related with a new bike launch. It is good to complement the raw data plot with a regression line corresponding to this linear model as done in the next chunk with the function geom_smooth(): ggplot(ebike_narrow) + geom_point(aes(x = temperature, y = cycles)) + geom_smooth(aes(x = temperature, y = cycles), method = &quot;lm&quot;) + geom_point(aes(x = temperature, y = cycles_mean), color = &quot;red&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) This visualization shows how a linear regression line adjusts to the data and we can see it is not passing exactly at the means of each treatment level. In the next steps we go into the functions underneath that are used to calculate the regression line. Linear models ebike_lm &lt;- lm(cycles ~ temperature, data = ebike_narrow) summary(ebike_lm) Call: lm(formula = cycles ~ temperature, data = ebike_narrow) Residuals: Min 1Q Median 3Q Max -43020 -12325 -1210 16710 33060 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 137620 41211 3.34 0.0036 ** temperature 2527 215 11.73 7.3e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 21500 on 18 degrees of freedom Multiple R-squared: 0.884, Adjusted R-squared: 0.878 F-statistic: 138 on 1 and 18 DF, p-value: 7.26e-10 This previous code chunk from the lab supervisor draft report is a linear model built with the variable temperature as a numeric vector. The R summary() function produces a specific output for linear models and a dedicated help explaining each output value can be accessed with ?summary.lm. Knowing that R uses specific “methods” to provide the summaries for many functions is useful to find their help pages and a way to list them is apropos(\"summary). In this case we see a high R-squared suggesting a very good fit and that the temperature is significant by looking at the 3 significance stars next to its p-value. Contrasts ebike_factor &lt;- ebike_narrow %&gt;% mutate(temperature = as_factor(temperature)) contrasts(ebike_factor$temperature) &lt;- contr.treatment attributes(ebike_factor$temperature) $levels [1] &quot;160&quot; &quot;180&quot; &quot;200&quot; &quot;220&quot; $class [1] &quot;factor&quot; $contrasts 2 3 4 160 0 0 0 180 1 0 0 200 0 1 0 220 0 0 1 The engineering team has selected to specify and control the temperature variable at specific levels in what is called a fixed effects model, limiting the conclusions to the levels tested. The lab supervisor updates his dataset by converting the temperature variable to a factor and explicitly establishes the factor contrasts with the contrasts() function. He selects cont.treatment. Looking into the attributes of the factor we see the matrix of contrasts. In many cases it is possible to skip this step as contr.treatment is default setting for the contrasts. This can be confirmed with getOption(\"contrasts\"). He can now establish a new linear model using the modified dataset. ebike_lm_factor &lt;- lm( cycles ~ temperature, data = ebike_factor ) summary(ebike_lm_factor) Call: lm(formula = cycles ~ temperature, data = ebike_factor) Residuals: Min 1Q Median 3Q Max -25400 -13000 2800 13200 25600 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 551200 8170 67.47 &lt; 2e-16 *** temperature2 36200 11553 3.13 0.0064 ** temperature3 74200 11553 6.42 8.4e-06 *** temperature4 155800 11553 13.49 3.7e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 18300 on 16 degrees of freedom Multiple R-squared: 0.926, Adjusted R-squared: 0.912 F-statistic: 66.8 on 3 and 16 DF, p-value: 2.88e-09 We see that from the first model to the second the R-squared has improved and that the model coefficients are slightly different. In R the model coefficients depend on the variable variable data type and on the contrasts setting. To obtain equivalent results with the different type coding it is necessary to carefully set the model contrasts. These differences are due to the calculation of different linear regression equations with different coefficients. It is important to be attentive before using whatever output the system is giving us. We can see the coefficients and use them to calculate the output with a matrix multiplication as follows: ebike_lm$coefficients (Intercept) temperature 137620 2527 ebike_lm$coefficients %*% c(1, 180) [,1] [1,] 592480 this shows that to calculate the output for an input of 180 we have 137’620 + 180 x 2’527 = 592’480. Making a zoom on the linear regression plot we see this passes slightly above the mean for the 180°C treatment level: ggplot(ebike_narrow) + geom_point(aes(x = temperature, y = cycles)) + geom_smooth(aes(x = temperature, y = cycles), method = &quot;lm&quot;) + geom_point(aes(x = temperature, y = cycles_mean), color = &quot;red&quot;) + scale_y_continuous(n.breaks = 20, labels = label_number(big.mark = &quot;&#39;&quot;)) + coord_cartesian(xlim = c(160, 180), ylim = c(520000, 620000)) + geom_hline(yintercept = 592480) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) On the other hand, when the temperature is coded as a factor we have the following coefficients and output calculation: ebike_lm_factor$coefficients (Intercept) temperature2 temperature3 temperature4 551200 36200 74200 155800 ebike_lm_factor$coefficients %*% c(1, 1, 0, 0) [,1] [1,] 587400 The output is slightly different: 551’200 + 1 x 36’200 = 587’400, corresponding exactly to the treatment mean for 180°C. More on this in the next section. Predict A model is useful for predictions. In a random effects model where conclusions can be applied to the all the population we can predict values at any value of the input variables. In that case reusing the model with temperature as a numeric vector we could have a prediction for various temperature values such as: ebike_new &lt;- tibble(temperature = c(180, 200, 210)) predict(ebike_lm, newdata = ebike_new) 1 2 3 592480 643020 668290 As mentioned in our case the team has selected a fixed effects model and in principle they should only draw conclusions at the levels at which the input was tested. We can check with predict() too that the predictions correspond exactly to the averages we’ve calculated for each level: ebike_new &lt;- data.frame(temperature = as_factor(c(&quot;180&quot;, &quot;200&quot;))) predict(ebike_lm_factor, newdata = ebike_new) 1 2 587400 625400 We find again exactly the same values calculated using the matrix multiplication of the linear regression coefficients with the input vector we used before. The predict() function has other advantages such as providing confidence intervals and taking into account the correct contrast coding, which will be explored in later case studies. The lab supervisor is now ready to assess the validity of the model. This is required before entering the main objective which is comparing the treatment means using an anova. To do this assessment the model he is going to do a residuals analysis. R provides direct plotting functions with the base and stats packages but he opted to break down the analysis and use custom the plots. He also uses some additional statistical tests to confirm our observations from the plots. He starts by loading the package broom which will help him retrieving the data from the lm object into a data frame. Model augment library(broom) ebike_aug &lt;- augment(ebike_lm_factor) %&gt;% mutate(index = row_number()) ebike_aug %&gt;% head() %&gt;% kable(align = &quot;c&quot;) cycles temperature .fitted .resid .hat .sigma .cooksd .std.resid index 575000 160 551200 23800 0.2 17571 0.13261 1.45665 1 542000 160 551200 -9200 0.2 18679 0.01982 -0.56307 2 530000 160 551200 -21200 0.2 17846 0.10522 -1.29752 3 539000 160 551200 -12200 0.2 18535 0.03485 -0.74668 4 570000 160 551200 18800 0.2 18069 0.08275 1.15063 5 565000 180 587400 -22400 0.2 17724 0.11747 -1.37096 6 Residuals analysis plots obtained with base R plot() function. In this unit each plot is generated individually with custom functions and a direct approach with based R is used in the next units. par(mfrow = c(2,2)) plot(ebike_lm_factor) dev.off() null device 1 A deep structural change has happened in R since the {tidyverse}. The original S and R creators had developed a language where matrices, vectors, lists and dataframes had equivalent importance. The output of a function was often a list with a specific S3 class comprising other vectors and data.frames inside. This allowed to use in a transparent way generic functions such as summary() to produce tailor made outputs because a method was working underneath. We’ve just seen an example of this with the lm() summary in the beginning of this case. For the plot() function there are more than a hundred different automatic plots as seen with apropos(\"plot\"). This is a very important difference as in the {tidyverse} we add layers to obtain the required plot. On the data side since {tidyverse} has been introduced we’ve seen an increasing importance of the dataframe, now replaced by the tibble. The augment() does exactly this, extracts the coefficients, residuals and other data from the model and stores it in a tibble format. This has the advantage of making it easier to integrate these functions with the other {tidyverse} functions and pipelines while still allowing to keep the methods approach. An interesting reading on this co-existence is available under tideness-modeling Timeseries plot ebike_aug %&gt;% ggplot(aes(x = index, y = .resid)) + geom_point(shape = 21, size = 2) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs( title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Residuals timeseries&quot;, y = &quot;Index&quot;, x = &quot;Fitted vaues&quot; ) Before drawing conclusions on the significance of the input variables it is important to assess the validity of the model. The anova assumptions are similar to the t.test assumptions discussed before. In fact the anova can be considered extension of the t.test to factors with more than 2 levels. These assumptions are the common ones coming from statistical inference principles and the central limit theorem: independent and random samples, normality of the distributions, equality of variances. These assumptions could be checked in each variable group but this would be very time consuming and not fully robust. A better way is to analyze the model residuals which are the deviations of each datapoint from the linear regression line. A first verification consists in confirming that the residuals have no patterns. This confirms that the sampling has been done randomly and there are none of the typical bias consisting in groups of values clustered from one operator the other or from one day to the other. This can be achieved with a residuals timeseries. If patterns emerge then there may be correlation in the residuals. For this plot we need to ensure that the order of plotting in the x axis corresponds exactly to the original data collection order. In this case the lab supervisor confirms that no specific pattern emerges from the current plot and the design presents itself well randomized. Autocorrelation test library(car) durbinWatsonTest(ebike_lm_factor) lag Autocorrelation D-W Statistic p-value 1 -0.53433 2.9609 0.104 Alternative hypothesis: rho != 0 As already stated visual observations can most of the times be complemented with a statistical test. In this case we can apply the durbinWatson test from the {car} package (Car stands for Companion to Applied Regression) Although the output shows Autocorrelation of -0.53 we have to consider that the p value is slightly higher than 0.05 thus there is not enough significance to say that there is autocorrelation. The result is not a complete clear cut the lab supervisor remains alert for coming verifications. Normality plot ebike_aug %&gt;% ggplot(aes(sample = .resid)) + geom_qq(shape = 21, size = 2) + geom_qq_line() + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs( title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - qq plot&quot;, y = &quot;Residuals&quot;, x = &quot;Fitted values&quot; ) A good next check is to verify that the residuals are normally distributed. As the sample size is relatively small it is better to use a qq plot instead of an histogram to assess the normality of the residuals. As we see on the plot values adhere to the straight line indicating an approximately normal distribution. In the fixed effects model we give more importance to the center of the values and here we consider acceptable that the extremes of the data tend to bend away from the straight line. This verification can be completed by a normality test. Normality test shapiro.test(ebike_aug$.resid) Shapiro-Wilk normality test data: ebike_aug$.resid W = 0.938, p-value = 0.22 For populations &lt; 50 use the shapiro-wilk normality test, Here p &gt; 0.05 indicates that the residuals do not differ significantly from a normally distributed population. Residuals-Fit plot ebike_aug %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point(shape = 21, size = 2) + geom_smooth(method = stats::loess, se = FALSE, color = &quot;red&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs( title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Residuals vs Fitted values&quot;, y = &quot;Residuals&quot;, x = &quot;Fitted values&quot; ) If the model is correct and the assumptions hold, the residuals should be structureless. In particular they should be unrelated to any other variable including the predicted response. A plot of the residuals against the fitted values should reveal such structures. In this plot we see no variance anomalies such as a higher variance for a certain factor level or other types of skweness. Standard Residuals-Fit plot ebike_aug %&gt;% ggplot(aes(x = .fitted, y = abs(.std.resid))) + geom_point(shape = 21, size = 2) + geom_smooth(method = stats::loess, se = FALSE, color = &quot;red&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Standardised Residuals vs Fitted values&quot;, y = &quot;Standardised Residuals&quot;, x = &quot;Fitted values&quot;) This Standardized residuals plot helps detecting outliers in the residuals (any residual &gt; 3 standard deviations is a potential outlier). The plot shows no outliers to consider in this DOE. Standard Residuals-Factor plot ebike_aug %&gt;% ggplot(aes(x = as.numeric(temperature), y = .std.resid)) + geom_point(shape = 21, size = 2) + geom_smooth(method = stats::loess, se = FALSE, color = &quot;red&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Linear model - Standardised Residuals vs Factor levels&quot;, y = &quot;Standardised Residuals&quot;, x = &quot;Factor levels&quot;) Besides being another support to detect outliers, this additional plot also helps seeing if the variance of the residuals is identical in this case between the factor levels. Homocedasticity bartlett.test(cycles ~ temperature, data = ebike_factor) Bartlett test of homogeneity of variances data: cycles by temperature Bartlett&#39;s K-squared = 0.433, df = 3, p-value = 0.93 A complement to the residuals-fit/residuals-factors plots is the equality of variances test. Tests for variance comparison have been introduced in the Direct Comparisons case studies but the var.test() cannot be used here. Here we have more than two levels for which the Bartlett test is most suited. The normal distribution of the residuals has already been confirmed. This test is sensitive to the normality assumption, consequently, when the validity of this assumption is doubtful, it should not be used and be replaced by the modified Levene test for example. Applying the test we obtain a p-value is P = 0.93 meaning we cannot reject the null hypothesis. In statistical terms, there is no evidence to counter the claim that all five variances are the same. This is the same conclusion reached by analyzing the plot of residuals versus fitted values. Outliers test outlierTest(ebike_lm_factor) No Studentized residuals with Bonferroni p &lt; 0.05 Largest |rstudent|: rstudent unadjusted p-value Bonferroni p 12 1.6488 0.11997 NA In a case where we were doubtful we could go further and make a statistical test to assess if a certain value was an outlier. Another useful test is available in the {car} package in this case to test outliers. We get a Bonferroni adjusted p value as NA confirming that there is no outlier in the data. Cooks distance ebike_aug %&gt;% ggplot(aes(x = index, y = .cooksd)) + geom_col(color = viridis(12)[4], fill = &quot;grey90&quot;) + geom_hline(yintercept = 1, color = &quot;red&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Residuals vs Leverage&quot;, x = &quot;Observation&quot;, y = &quot;Cooks distance&quot;) Cooks distance is a complementary analysis to the residuals that can help identify specific data points that could have a strong influence in the model. Various cutoff points are suggested in the literature and we opted here for 1 following the short Wikipedia article on the topic cooks distance R-squared summary(ebike_lm_factor)$r.squared [1] 0.92606 A final input in the draft report of the ebike hardening linear model is the R-squared. When looking into the results the engineering team is suspicious. In this case 93% of the output is explained by input and a model with such a good fit should raise questions. Our lab supervisor is also not comfortable the residuals analysis has not shown any evidence of something wrong with the model so he decides to quickly calculate it “by hand.” He knows that the R-squared, or coefficient of determination is obtained from the ratio between the residuals variance and the output variable variance showing exactly the proportion between the two and he gets its straight away from R using the data already available: ebike_aug %&gt;% summarise(cycles_var = var(cycles), residuals_var = var(.resid)) %&gt;% mutate(Rsquared = 1 - residuals_var/cycles_var) %&gt;% pull(Rsquared) [1] 0.92606 Remembering the original linear regression plot from the beginning of the report he accepts this must not be so far away. It was clear that the temperature had a strong impact on the number of cycles and the variability for each level was small in the end. He accepts to leave as it is for now waiting for upcoming analysis of variance to see additional details. Effects significance The commercial introduction of the new e-bike model is approaching soon and production is expected to start in a couple of months. The engineering team is getting impatient because the parameters for the frame thermal treatment are not yet defined. The engineering head call for a second meeting to review once more the DoE outputs. The lab supervisor reopens his Rmd report tries to go beyond the linear model discussed before. He created raw data plots with dots on individual data points but now he thinks it is important to have a view on the data distribution and some summary statistics. For that he prepares a box plot: ggplot( ebike_factor, aes(x = temperature, y = cycles, fill = temperature)) + geom_boxplot() + scale_fill_viridis_d(option = &quot;D&quot;, begin = 0.5) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Raw data plot&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) They have been doing so many experiments that sometimes it gets hard to remember which variables have been tested in which experiment. This plot reminds him that this test consisted simply on 1 input variable with several levels - the temperature and one continuous dependent variable - the number of cycles to failure. The plots shows clearly that the distributes are quite apart from each other in spite of the slight overlap between the first three groups. The underlying question is: are the different levels of temperature explaining the different results in resistance to fatigue? to confirm that means of those groups are statistically different from each other he knows he can use the analysis of variance. The name is a bit misleading since he want to compare means…but this name is historical and comes from the way the approach has evolved. The anova as it is called is similar to the t-test but is extended. Using all pair wise t-tests would mean more effort and increase the type I error. The anova main principle is that the the total variability in the data, as measured by the total corrected sum of squares, can be partitioned into a sum of squares of the differences between the treatment averages and the grand average plus a sum of squares of the differences of observations within treatments from the treatment average. The first time he read this explanation it seemed complex but he understood better on seeing a simple hand made example on the kahn academy - anova. Anova ebike_aov_factor &lt;- aov(ebike_lm_factor) summary(ebike_aov_factor) Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 3 6.69e+10 2.23e+10 66.8 2.9e-09 *** Residuals 16 5.34e+09 3.34e+08 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In R the anova is built by passing the linear model object to the anova() or aov() functions. The output of the first is just the anova table, the output of the second function is a complete list with the full lm model inside. The R anova output gives the Mean Square for the factor and for the residuals. In this case the between-treatment mean square is much larger than the within-treatment or residuals mean square. This suggests that it is unlikely that the treatment means are equal. The p is extremely small thus we have basis to reject the null hypothesis and conclude that the means are significantly different. In the mean while the lab supervisor has gathered data on a similar experiment done in another material for that seems to be less sensitive the the treatment temperature. He uploads this data and assigns it to a dataset called ebike_hardning2 and plots another box plot. ebike_narrow2 &lt;- ebike_hardening2 %&gt;% pivot_longer( cols = starts_with(&quot;g&quot;), names_to = &quot;observation&quot;, values_to = &quot;cycles&quot; ) %&gt;% group_by(temperature) %&gt;% mutate(cycles_mean = mean(cycles)) %&gt;% ungroup() ebike_factor2 &lt;- ebike_narrow2 ebike_factor2$temperature &lt;- as.factor(ebike_factor2$temperature) ggplot(ebike_factor2, aes(x = temperature, y = cycles, fill = temperature)) + geom_boxplot() + scale_y_continuous(n.breaks = 10) + scale_fill_viridis_d(option = &quot;A&quot;, begin = 0.5) + theme(legend.position = &quot;none&quot;) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Boxplot of frame aging resistance&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) Effectively within group variation is larger and groups overlap more. A new anova gives a p value of 0.34 supporting the assumption of no significant difference between the means of the treatment levels. ebike_lm_factor2 &lt;- lm(cycles ~ temperature, data = ebike_factor2) ebike_aov_factor2 &lt;- aov(ebike_lm_factor2) summary(ebike_aov_factor2) Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 3 1.48e+09 4.92e+08 1.2 0.34 Residuals 16 6.55e+09 4.10e+08 Pairwise comparison ebike_tukey &lt;- TukeyHSD(ebike_aov_factor, ordered = TRUE) head(ebike_tukey$temperature) %&gt;% kable(align = &quot;c&quot;, caption = &quot;tukey test on e-bike frame hardening process&quot;, booktabs = T) Table 6: tukey test on e-bike frame hardening process diff lwr upr p adj 180-160 36200 3145.6 69254 0.02943 200-160 74200 41145.6 107254 0.00005 220-160 155800 122745.6 188854 0.00000 200-180 38000 4945.6 71054 0.02160 220-180 119600 86545.6 152654 0.00000 220-200 81600 48545.6 114654 0.00001 Back to the main test the lab supervisor wants to see if all levels are significantly different from each other. As discusses the anova indicates that there is a difference in the treatment means but it won’t indicate which ones and doing individual t.tests has already been discarded. It is possible to get a direct one to one comparison of means with TukeyHSD() from {stats}. The test also provides a confidence interval for each difference. Most importantly it provides us with the p value to help us confirm the significance of the difference and conclude factor level by factor level which differences are significant. Additionally we can also obtain the related plot with the confidence intervals plot(ebike_tukey) In the case of the frames thermal treatment all levels bring a specific impact on the lifecycle as we can see from the p values all below 0.05 and from the fact that no confidence interval crosses zero (there are no differences that could have a chance of being zero). Least significant difference library(agricolae) ebike_LSD &lt;- LSD.test( y = ebike_lm_factor, trt = &quot;temperature&quot; ) A useful complement to Tukey’s test is the calculation of Fisher’s Least Significant differences. The Fisher procedure can be done in R with the LSD.test() from the {agricolae} package. The first important ouput is precisely the least significant difference which is the smallest the difference between means (of the the life cycles) that can be considered significant. This is indicated in the table below with the value LSD = 24’492. head(ebike_LSD$statistics) %&gt;% kable(align = &quot;c&quot;, caption = &quot;Fisher LSD procedure on e-bike frame hardening: stats&quot;, booktabs = T) Table 7: Fisher LSD procedure on e-bike frame hardening: stats MSerror Df Mean CV t.value LSD 333700000 16 617750 2.9571 2.1199 24492 Furthermore it gives us a confidence intervals for each treatment level mean: head(ebike_LSD$means) %&gt;% select(-Q25, -Q50, -Q75) %&gt;% kable(align = &quot;c&quot;, caption = &quot;Fisher LSD procedure on e-bike frame hardening: means&quot;, booktabs = T) Table 8: Fisher LSD procedure on e-bike frame hardening: means cycles std r LCL UCL Min Max 160 551200 20017 5 533882 568518 530000 575000 180 587400 16742 5 570082 604718 565000 610000 200 625400 20526 5 608082 642718 600000 651000 220 707000 15248 5 689682 724318 685000 725000 We can see for example that for temperature 180 °C the lifecyle has an average of 587’400 (has he had calculated before) with a probability of 95% of being between 570’082 and and 604’718 cycles. Another useful outcome is the creation of groups of significance. head(ebike_LSD$groups) %&gt;% kable(align = &quot;c&quot;, caption = &quot;Fisher LSD procedure on e-bike frame hardening: groups&quot;, booktabs = T) Table 9: Fisher LSD procedure on e-bike frame hardening: groups cycles groups 220 707000 a 200 625400 b 180 587400 c 160 551200 d In this case as all level means are statistically different they all show up in separate groups, each indicated by a specific letter. Finally we can use plot() which calls the method plot.group() from the same package. This allows us to provide as input the desired argument for the error bars. plot( ebike_LSD, variation = &quot;SE&quot;, main = &quot;e-bike hardening\\nMeans comparison&quot; ) Strangely the package plot doesn’t have the option to plot error bars with LSD and the lab supervisor decides to make a custom plot: ebike_factor %&gt;% group_by(temperature) %&gt;% summarise(cycles_mean = mean(cycles), cycles_lsd = ebike_LSD$statistics$LSD) %&gt;% ggplot(aes(x = temperature, y = cycles_mean, color = temperature)) + geom_point(size = 2) + geom_line() + geom_errorbar(aes(ymin = cycles_mean - cycles_lsd, ymax = cycles_mean + cycles_lsd), width = .1) + scale_y_continuous(n.breaks = 10, labels = label_number(big.mark = &quot;&#39;&quot;)) + scale_color_viridis_d(option = &quot;C&quot;, begin = 0.1, end = 0.8) + annotate(geom = &quot;text&quot;, x = Inf, y = -Inf, label = &quot;Error bars are +/- 1xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + labs(title = &quot;e-bike frame hardening process&quot;, subtitle = &quot;Boxplot of frame aging resistance&quot;, x = &quot;Furnace Temperature [°C]&quot;, y = &quot;Cycles to failure [n]&quot;) The plot shows some overlap between the levels of 160 and 180 and again between 180 and 200. When looking a the Tukey test outcome we see that the p value of these differences is close to 0.05. Presenting all these statistical findings to the team they end up agreeing that in order to really improve the resistance they should consider a jump from 160 to 200°C in the thermal treatment. As often with statistical tools, there is debate on the best approach to use. We recommend to combine the Tukey test with the Fisher’s LSD. The Tukey test giving a first indication of the levels that have an effect and calculating the means differences and the Fisher function to provide much more additional information on each level. To be considered in each situation the slight difference between the significance level for difference between means and to decide if required to take the most conservative one. To go further in the Anova F-test we recommend this interesting article from Minitab (2016). Interactions Case study: solarcell output test The countdown to leave fossil fuel has started as many companies have adopted firm timelines for 100% renewable energy sourcing. Solar energy is a great candidate but solar cell efficiency is a great challenge. Although it has been progressing steadily since more than four decades yields can still be considered low. A global manufacturing company of solar cells is looking to push the boundaries with a new generation of materials and grab another pie of the global market. Model formulae solarcell_formula &lt;- formula( output ~ temperature * material ) In previous case studies input factors has been put directly in the arguments of the lm() function by using the inputs and outputs and relating them with the tilde ~ sign. The cases were simple with only one factor but in most DoEs we want to have many factors and decide which interactions to keep or drop. Here we’re looking a bit more into detail in how to express this. When we pass an expression to the formula() function we generate an object of class formula and at that time some calculations are done in background to prepare the factors for the linear model calculation. Looking at the formula class and attributes we have: class(solarcell_formula) [1] &quot;formula&quot; attributes(terms(solarcell_formula))$factors temperature material temperature:material output 0 0 0 temperature 1 0 1 material 0 1 1 We can see that the expression has been extended. Although we have only given as input the product of the factors we can see that an interaction term temperature:material has been generated. We also see the contrasts matrix associated. There is a specific syntax to specify the formula terms using *,+ and other symbols. As always it is good to consult the function documentation with ?formula. In the solar cell manufacturing company mentioned before the R&amp;D team is working a new research project with the objective of understanding the output in [kWh/yr equivalent] of a new solar cell material at different ambient temperatures. Their latest experiment is recorded in an R dataset with the name solarcell_output: solarcell_output %&gt;% head(5) %&gt;% kable(align = &quot;c&quot;) material run T-10 T20 T50 thinfilm 1 130 34 20 thinfilm 2 74 80 82 thinfilm 3 155 40 70 thinfilm 4 180 75 58 christaline 1 150 136 25 As often this data comes in a wide format and the first step we’re doing is to convert it into a long format and to convert the variables to factors. solarcell_factor &lt;- solarcell_output %&gt;% pivot_longer( cols = c(&quot;T-10&quot;, &quot;T20&quot;, &quot;T50&quot;), names_to = &quot;temperature&quot;, values_to = &quot;output&quot; ) %&gt;% mutate(across(c(material, temperature), as_factor)) The experiment has consisted in measuring the output at three different temperature levels on three different materials. The associated linear model can be obtained with: solarcell_factor_lm &lt;- lm( formula = solarcell_formula, data = solarcell_factor ) summary(solarcell_factor_lm) Call: lm(formula = solarcell_formula, data = solarcell_factor) Residuals: Min 1Q Median 3Q Max -60.75 -14.63 1.38 17.94 45.25 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 134.75 12.99 10.37 6.5e-11 *** temperatureT20 -77.50 18.37 -4.22 0.00025 *** temperatureT50 -77.25 18.37 -4.20 0.00026 *** materialchristaline 21.00 18.37 1.14 0.26311 materialmultijunction 9.25 18.37 0.50 0.61875 temperatureT20:materialchristaline 41.50 25.98 1.60 0.12189 temperatureT50:materialchristaline -29.00 25.98 -1.12 0.27424 temperatureT20:materialmultijunction 79.25 25.98 3.05 0.00508 ** temperatureT50:materialmultijunction 18.75 25.98 0.72 0.47676 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 26 on 27 degrees of freedom Multiple R-squared: 0.765, Adjusted R-squared: 0.696 F-statistic: 11 on 8 and 27 DF, p-value: 9.43e-07 We’re going to go more in details now to validate the model and understand the effects and interactions of the different factors. Residuals standard error pluck(summary(solarcell_factor_lm), &quot;sigma&quot;) [1] 25.985 Besides the R-squared discussed previously in the linear models unit there is another useful indicator of the quality of the fit which is the Residuals Standard Error RSE. It provides the magnitude of a typical residuals. This value is also given directly as output of the model summary and is 26 in this case. Like the R-squared is better when we know how it is calculated and once we’re at ease with manipulating the model data either with {stats} or {broom} it is possible to with a few steps check see how this is done. sqrt(sum(solarcell_factor_lm$residuals ^ 2) / df.residual(solarcell_factor_lm)) [1] 25.985 The exact value is 25.985 confirming the value extracted from the summary with the pluck() function from {purrr}. Residuals summary par(mfrow = c(2,3)) plot(solarcell_factor_lm$residuals) plot(solarcell_factor_lm, which = 2) plot(solarcell_factor_lm, which = c(1, 3, 5)) plot(solarcell_factor_lm, which = 4) dev.off() null device 1 As the residuals analysis has been discussed in detail including custom made plots and statistical tests in the linear models unit, the assessment is done here in a summarized manner with a grouped output of all residuals plots. The qq plot presents good adherence to the center line indicating a normal distribution; the residuals versus fit presents a rather symmetrical distribution around zero indicating equality of variances at all levels and; the scale location plot though, shows a center line that is not horizontal which suggests the presence of outliers; in the Residuals versus fit we can effectively sense the Residuals Standard Error of 26. Interaction plot interaction.plot( type = &quot;b&quot;, col = viridis(12)[4], x.factor = solarcell_factor$temperature, trace.factor = solarcell_factor$material, fun = mean, response = solarcell_factor$output, trace.label = &quot;Material&quot;, legend = TRUE, main = &quot;Temperature-Material interaction plot&quot;, xlab = &quot;temperature [°C]&quot;, ylab = &quot;output [kWh/yr equivalent]&quot; ) In order to understand the behavior of the solar cell materials in the different temperature conditions the R&amp;D team is looking for a plot that presents both factors simultaneous. Many different approaches are possible in R and here the team has selected the most basic one, the interactionplot() from the {stats} package. Although simple several findings can already be extracted from this plot. They get the indication of the mean value of the solar cell output for the different materials at each temperature level. Also we see immediately that batteries tend to last longer at lower temperatures and this for all material types. We also see that there is certainly an interaction between material and temperature as the lines cross each other. Anova with interactions anova(solarcell_factor_lm) Analysis of Variance Table Response: output Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 2 39119 19559 28.97 1.9e-07 *** material 2 10684 5342 7.91 0.002 ** temperature:material 4 9614 2403 3.56 0.019 * Residuals 27 18231 675 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Continuing the analysis started in the interaction plot the R&amp;D team checks the anova output. Like in the lm summary output, the stars in front of the p value of the different factors indicate that the effects are statistically different. Three stars for temperature corresponding to an extremely low p value indicating that the means of the output at the different levels of temperature are different. This confirms that temperature has an effect on output power. The material effect has a lower significance but is also clearly impacting cell power output. Finally it is confirmed that there is an interaction between temperature and material as the temperature:material term has a p value of 0.019 which is lower than the typical threshold of 0.05. Looking into the details interaction comes from the fact that increasing temperature from 10 to 20 decreases output for the thinfilm but is not yet impacting the output for multijunction film. For multijunction it is needed to increase even further the temperature to 50°C to see the decrease in the output. Before closing the first DOE analysis meeting the R&amp;D team discusses what would have been take-aways if the interaction had not put in the model. As they use more and more R during their meetings and do the data analysis on the sport they simply create another model without the temperature:material term in the formula: solarcell_factor_lm_no_int &lt;- lm( output ~ temperature + material, data = solarcell_factor) summary(solarcell_factor_lm_no_int) Call: lm(formula = output ~ temperature + material, data = solarcell_factor) Residuals: Min 1Q Median 3Q Max -54.39 -21.68 2.69 17.22 57.53 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 122.5 11.2 10.97 3.4e-12 *** temperatureT20 -37.2 12.2 -3.04 0.0047 ** temperatureT50 -80.7 12.2 -6.59 2.3e-07 *** materialchristaline 25.2 12.2 2.06 0.0482 * materialmultijunction 41.9 12.2 3.43 0.0017 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 30 on 31 degrees of freedom Multiple R-squared: 0.641, Adjusted R-squared: 0.595 F-statistic: 13.9 on 4 and 31 DF, p-value: 1.37e-06 Residual standard error is up from 26 to 30 which shows a poorer fit but R-square is only down from 76.5% to 64.1% which is still reasonably high. They apply the anova on this new model: anova(solarcell_factor_lm_no_int) Analysis of Variance Table Response: output Df Sum Sq Mean Sq F value Pr(&gt;F) temperature 2 39119 19559 21.78 1.2e-06 *** material 2 10684 5342 5.95 0.0065 ** Residuals 31 27845 898 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output still confirms the significance of the effects of the factors but the residuals analysis raises other concerns: par(mfrow = c(2,3)) plot(solarcell_factor_lm_no_int$residuals) plot(solarcell_factor_lm_no_int, which = 2) plot(solarcell_factor_lm_no_int, which = c(1, 3, 5)) plot(solarcell_factor_lm_no_int, which = 4) dev.off() null device 1 They see in the Residuals vs Fitted a clear pattern with residuals moving from positive to negative and then again to positive along the fitted values axis which indicates that there is an interaction at play. Another concern comes from the Residuals versus Factor levels where at 10°C some residuals go beyond 2 standard deviations. The model with the interaction is clearly preferred in this case. Covariance solarcell_fill %&gt;% head(5) %&gt;% kable() material output fillfactor multijunction_A 108 20 multijunction_A 123 25 multijunction_A 117 24 multijunction_A 126 25 multijunction_A 147 32 Solarcell experiments continue as the R&amp;D project on new materials progresses. Any increase in the output, which is measured in [kWh/yr equivalent will bring a competitive advantage to the company. The previous meeting outcome made the R&amp;D team select the multijunction material as the best candidate for the next round of tests. A new experiment has been designed but the team needs to go deeper in the understanding on how to improve the power output. Besides temperature and material there seems to be another variable at play: the fill factor. This seems to be a complex technical topic but all experts agree that this is influencing the behavior of the cell. The fill factor depends on the electrical circuit configuration and the output seems to be correlated with it. Until now the team has not been able to control the fill factor. The table just presented shows the value of fill factor collected for each cell tested together with the measured output. A Data Scientist from the center recommends to use an analysis of covariance (ancova) which can be useful in situations where a continuous variable may be influencing the measured value. He calls this a covariate. In such specific case this approach provides better results than the analysis of variance (anova) allowing for a more accurate assessment of the effects of the categorical variable. In this case it can remove the effect of the fill factor in the output when we want to compare the different materials. It is nevertheless important to ensure the basic assumption that the continuous variable is independent from the factor to be analyses, in this case that the material is not influencing the fill factor. A good explanation and a similar case (without R) can be seen on page 655 of Montgomery (2012). solarcell_fill %&gt;% ggplot(aes(x = fillfactor, y = output)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_industRial() + labs( title = &quot;The solarcell output test&quot;, subtitle = &quot;Output vs Fill Factor&quot;, x = &quot;Fill factor [%]&quot;, y = &quot;Output&quot; ) Correlation test cor.test( solarcell_fill$output, solarcell_fill$fillfactor, method = &quot;pearson&quot; ) Pearson&#39;s product-moment correlation data: solarcell_fill$output and solarcell_fill$fillfactor t = 9.8, df = 13, p-value = 2.3e-07 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.82100 0.97976 sample estimates: cor 0.93854 The next step is to confirm the correlation between the continuous input variable and the output and the cor.test() from the {stats} package is perfectly suited for this. The extremely high value of 93% confirms what was very visible in the scatterplot. Going further and using the approach from (Broc 2016) we’re going to facet the scatterplots to assess if the coefficient of the linear regression is similar for all the levels of the fillfactor: solarcell_fill %&gt;% ggplot(aes(x = fillfactor, y = output)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_wrap(vars(material)) + theme_industRial() + labs( title = &quot;The solarcell output test&quot;, subtitle = &quot;Output vs Fill Factor, by material type&quot;, x = &quot;Fill factor [%]&quot;, y = &quot;Output&quot; ) The linear regression plots split by material show that from one material to the other the relationship between output and fillfactor is equivalent. Not only increasing fill factor increase output the degree to which this takes place is similar as we can see by the slopes of the plot. Care needs to be taken because the number of points is very small. If required it is always possible to do individual correlation test and/or do a statistical test between slopes. Now things are ready to the ancova itself. Ancova solarcell_ancova &lt;- aov( formula = output ~ fillfactor + material, data = solarcell_fill ) solarcell_aov &lt;- aov( output ~ material, data = solarcell_fill ) Although the team had been using R often the case of the ancova had not yet came up so it was up to the Data Scientist to do this analysis. In R the ancova can be done with the same function as the anova, the aov() function from {stats} but there’s a specific way to establish the formula which he has obtained from Datanovia - Ancova: the covariate is the first input and there must be interaction between the two inputs, thus the plus sign only. As with contrasts, any little mistake in the syntax may produce very different results so it requires great care and often confirmation of calculation with an existing well know case. summary(solarcell_ancova) Df Sum Sq Mean Sq F value Pr(&gt;F) fillfactor 1 2746 2746 119.93 3e-07 *** material 2 120 60 2.61 0.12 Residuals 11 252 23 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(solarcell_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) material 2 1264 632 4.09 0.044 * Residuals 12 1854 155 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The aov() summary output looks exactly the same for both analysis but the in the first output corresponding to the ancova the material mean square has been adjusted for regression and is smaller. It is also noticeable that the residuals are smaller in the ancova which confirm that the approach has helped reducing the error variability. Finally the most important observation is that the conclusions would have been just the opposite if the Data Scientist had not recommended the ancova. In fact in the anova would have shown that the material influences the output while when removing the influence of the fill factor the ancova ends up showing that there is no influence. This is visible in the p value which is above 0.05 in the ancova and below 0.05 in the anova. As next steps the R&amp;D team decides to tackle this fill factor issue and get it into control. Reducing fill factor variability within each material will certainly reduce the variability in the power output. I would also make upcoming experiments simpler and more easily comparable. General designs General factorial designs require teams to put together a wealth of knowledge of which some has been already applied in previous case studies or is referred in the bibliography and glossary. This comprises things like root cause analysis, linear models and analysis of variance all coherently articulated in a well though project with clear objectives. The building blocks discussed so far relate to a limited number of input factors and levels and exclusively a single output variable. Model validation and interactions are needed tools in all cases and once these are all mastered it becomes possible to consider situations with many variables, many outputs and higher level interactions. These arrangements become extremely powerful and allow to handle complex real life situations such as the design of a new system with dozens of features that relate with each other or the optimization of a manufacturing process where the amount of data generated is very large but the testing time and cost are very high. At this moment considerations of trial quantities optimization enter at play. In our case studies a run represents a unique combination of the factors and a replicate an independent repetition of a run. This leads to the notion of trials corresponding to the multiplication of the number of runs by the number of replicates. For small designs it is possible to calculate the number of trials by simply multiplying the number of levels of each factor. If we have for instance 1 factor with 3 levels and 2 factors with 4 levels then we have \\(3 \\times 4 \\times 4 = 48\\) runs which corresponds to the number of distinct combinations. Then we have to multiply by the number of replicates per run to get the number of trials, e.g \\(48 \\times 3 = 144\\) trials. For a very high number of factors and levels where this way of calculating may not be practical the total number of trials is given by \\(l^k\\) where \\(l\\) is the number of levels and \\(k\\) the number of input factors. With this approach a design with 4 factors of 2 levels gives \\(2^4 = 2 \\times 2 \\times 2 \\times 2 = 16\\) runs and if each has 5 replicates there are \\(16 \\times 5 = 80\\) trials to be executed. If more factors with a different number of levels are added, the total number of trials is calculated by multiplying both groups: \\(l_{1}^{k_{1}}\\) \\(\\times\\) \\(l_{2}^{k_{2}}\\). Continuing the previous example, if 3 additional factors with 4 levels each were added, all with 5 replicates, the total number of trials would be expressed as follows: \\(2^{4} \\times 4^{3} \\times 5 = 5120\\) trials, which is a very high number in most industrial cases and would require optimization techniques which will be discussed in later units. Factorial design Case study: juice bottling plant In a juice producing plant a new fast dry matter content measurement device from the supplier DRX has been recently put in operation but the Quality Assurance Head has raised concerns on a bias with the reference method. The quality team established DoE to assess several potential causes such as the dry matter content and juice residue particle size. library(DoE.base) juice_doe &lt;- fac.design( randomize = FALSE, factor.names = list( product = c(&quot;beetroot&quot;, &quot;apple&quot;, &quot;carrot&quot;), drymatter_target = c(10, 15, 20), part = c(1, 2, 3), speed = c(20, 25), particle_size = c(250, 300)) ) Although the Calibration essay discussed in the MSA unit has shown a bias during the acceptance phase the Factory Management has opted to put it in production. The reduction in measurement time is significant and Supply Chain is putting pressure to increase volumes in a context where on-line sales rocket sky high. The Quality Manager understands all this but he’s concern of having some kind of kickback. Although he’s not expecting any kind of consumer complain, dry matter levels are somehow loosely related with some sort claims and regulatory limits. To dig deeper and understand how to minimize this bias he has asked one of his team members to come up with an experiment design. He would like something that combines all factors mentioned by the team as potential root causes for the bias. After a short brainstorming between the production and quality teams several potential causes for bias were: drymatter level, the speed of filling and the powder particle size. This lead to a mid size experiment design with three products, three levels of drymatter, two line speed levels and two particle size levels. When the number of factors and levels is limited it is possible to recur to existing experiment designs and pre-filled Yates tables. In this case the quality analyst had been trying with R and found a package called {DoE.base} which generates such designs automatically with the function fac.desig. The output generated by this function is more than just a tibble, it belongs to a specific class called designand has other attributes just like an lm or aov S3 objects. The care given by the package authors becomes visible when using an R generic function such as summary() with such object and get as return a tailor made output, in this case showing the levels of the different factors of our design: class(juice_doe) [1] &quot;design&quot; &quot;data.frame&quot; summary(juice_doe) Call: fac.design(randomize = FALSE, factor.names = list(product = c(&quot;beetroot&quot;, &quot;apple&quot;, &quot;carrot&quot;), drymatter_target = c(10, 15, 20), part = c(1, 2, 3), speed = c(20, 25), particle_size = c(250, 300))) Experimental design of type full factorial 108 runs Factor settings (scale ends): product drymatter_target part speed particle_size 1 beetroot 10 1 20 250 2 apple 15 2 25 300 3 carrot 20 3 In the summary() output we can see the plan factors with 3 products, 3 levels of dry matter target, 2 levels for speed and 2 levels for particle size. Using this the team has simple copied the experiment plan to an spreadsheet to collect the data: juice_doe %&gt;% write_clip() and after a few days the file completed with the test results came back ready for analysis juice_drymatter %&gt;% head() %&gt;% kable() product drymatter_TGT speed particle_size part drymatter_DRX drymatter_REF apple 10 20 250 1 9.80 10.05 apple 10 20 250 2 9.82 10.05 apple 10 20 250 3 9.82 10.05 beetroot 10 20 250 1 9.79 10.03 beetroot 10 20 250 2 9.75 10.03 beetroot 10 20 250 3 9.77 10.03 and the only thing quality analyst had to add was an extra column to calculate the bias: juice_drymatter &lt;- juice_drymatter %&gt;% mutate(bias = drymatter_DRX - drymatter_REF) Main effects plots Figure 1: Main drymatter_TGT_plot &lt;- juice_drymatter %&gt;% group_by(drymatter_TGT) %&gt;% summarise(bias_m_drymatter = mean(bias)) %&gt;% ggplot(aes(x = drymatter_TGT, y = bias_m_drymatter)) + geom_point() + geom_line() + coord_cartesian( xlim = c(9,21), ylim = c(-1,0), expand = TRUE) + labs( title = &quot;Juice bottling problem&quot;, subtitle = &quot;Main effects plots&quot;, x = &quot;Drymatter TGT [%]&quot;, y = &quot;Average bias [g]&quot; ) As the number of factors and levels of a design increase, more thinking is required to obtain good visualisation of the data. Main effects plots consist usually of a scatterplot representing the experiment output as a function of one of the inputs. This first plot consists of the mean bias as a function of the dry matter for each of the 3 levels tested. As the DOE has 3 factors, three plots like this are required in total. The Quality Analyst builds the remaining two plots and then groups them all together in a single output with the package {patchwork}. This is made by simply putting + between the plots. particle_size_plot &lt;- juice_drymatter %&gt;% group_by(particle_size) %&gt;% summarise(particle_size_bias_mean = mean(bias)) %&gt;% ggplot(aes(x = particle_size, y = particle_size_bias_mean)) + geom_point() + geom_line() + coord_cartesian( xlim = c(240,310), ylim = c(-1,0), expand = TRUE) + labs( x = &quot;Particle Size&quot;, y = &quot;Average bias [g]&quot; ) speed_plot &lt;- juice_drymatter %&gt;% group_by(speed) %&gt;% summarise(speed_bias_mean = mean(bias)) %&gt;% ggplot(aes(x = speed, y = speed_bias_mean)) + geom_point() + geom_line() + coord_cartesian( xlim = c(19, 26), ylim = c(-1,0), expand = TRUE) + labs( x = &quot;Speed&quot;, y = &quot;Average bias [g]&quot; ) drymatter_TGT_plot + particle_size_plot + speed_plot Main effects plots give important insights in to the experiment outcomes and this even before any statistical analysis with a linear model and anova. From these three plots the Quality Analyst already takes the following observations for her report: bias increases in negative direction as dry matter content increases bias increases in positive direction as particle size increases bias is not influence by line speed Interaction plots (custom) drymatter_TGT_particle_size_plot &lt;- juice_drymatter %&gt;% mutate(particle_size = as_factor(particle_size)) %&gt;% group_by(drymatter_TGT, particle_size) %&gt;% summarise(drymatter_bias_mean = mean(bias), drymatter_bias_sd = sd(bias)) %&gt;% ggplot(aes(x = drymatter_TGT, y = drymatter_bias_mean, color = particle_size, linetype = particle_size)) + geom_point(aes(group = particle_size), size = 2) + geom_line(aes(group = particle_size, linetype = particle_size)) + scale_linetype(guide=FALSE) + geom_errorbar(aes( ymin = drymatter_bias_mean - 2 * drymatter_bias_sd, ymax = drymatter_bias_mean + 2 * drymatter_bias_sd, width = .5 )) + scale_color_viridis_d(option = &quot;C&quot;, begin = 0.3, end = 0.7, name = &quot;Particle size&quot;) + coord_cartesian( xlim = c(9,21), ylim = c(-1,0), expand = TRUE) + annotate(geom = &quot;text&quot;, x = Inf, y = 0, label = &quot;Error bars are +/- 2xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + labs( title = &quot;Juice bottling problem&quot;, subtitle = &quot;Interaction plots&quot;, x = &quot;Drymatter target&quot;, y = &quot;Average bias deviation [g]&quot; ) + theme(legend.justification=c(1,0), legend.position=c(1,0)) Now to look deeper and she’s preparing interaction plots. She wants to understand if factors combine in unexpected ways at certain levels. In designs like these with 3 factors we have 3 possible interactions (A-B, A-C, B-C) corresponding the the possible combination between them. It is important to keep in mind that at least two replicates by run are needed to be able determine the sum of squares due to error, this if all possible interactions are to be included in the model. As the plan is a full factorial plan and there are more than 2 replicates, all factor combinations are resolved and can be assessed for their significance. The interaction plots show precisely such combinations, two at a time against the output. The first one Dry matter target - Particle Size being ready she moves to the next two: Dry matter target - Speed and Speed - Particle Size. drymatter_TGT_speed_plot &lt;- juice_drymatter %&gt;% mutate(speed = as_factor(speed)) %&gt;% group_by(drymatter_TGT, speed) %&gt;% summarise(drymatter_bias_mean = mean(bias), drymatter_bias_sd = sd(bias)) %&gt;% ggplot(aes(x = drymatter_TGT, y = drymatter_bias_mean, color = speed)) + geom_point(aes(group = speed), size = 2) + geom_line(aes(group = speed, linetype = speed)) + scale_linetype( guide=FALSE) + scale_color_viridis_d(option = &quot;C&quot;, begin = 0.3, end = 0.7, name = &quot;Speed&quot;) + geom_errorbar(aes( ymin = drymatter_bias_mean - 2 * drymatter_bias_sd, ymax = drymatter_bias_mean + 2 * drymatter_bias_sd, width = .5 )) + coord_cartesian( xlim = c(9, 21), ylim = c(-1,0), expand = TRUE) + annotate(geom = &quot;text&quot;, x = Inf, y = 0, label = &quot;Error bars are +/- 2xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + labs( x = &quot;Dry matter target&quot;, y = &quot;Average bias deviation [g]&quot; ) + theme(legend.justification=c(1,0), legend.position=c(1,0)) speed_particle_size_plot &lt;- juice_drymatter %&gt;% mutate(particle_size = as_factor(particle_size)) %&gt;% group_by(speed, particle_size) %&gt;% summarise(drymatter_bias_mean = mean(bias), drymatter_bias_sd = sd(bias)) %&gt;% ggplot(aes(x = speed, y = drymatter_bias_mean, color = particle_size)) + geom_point(aes(group = particle_size), size = 2) + geom_line(aes(group = particle_size, linetype = particle_size)) + scale_linetype(guide=FALSE) + scale_color_viridis_d(option = &quot;C&quot;, begin = 0.3, end = 0.7, name = &quot;Particle size&quot;) + geom_errorbar(aes( ymin = drymatter_bias_mean - 2 * drymatter_bias_sd, ymax = drymatter_bias_mean + 2 * drymatter_bias_sd, width = .5 )) + coord_cartesian( xlim = c(19, 26), ylim = c(-1,0), expand = TRUE) + annotate(geom = &quot;text&quot;, x = Inf, y = 0, label = &quot;Error bars are +/- 2xSD&quot;, hjust = 1, vjust = -1, colour = &quot;grey30&quot;, size = 3, fontface = &quot;italic&quot;) + labs( x = &quot;Speed&quot;, y = &quot;Average bias deviation [g]&quot; ) + theme(legend.justification=c(1,0), legend.position=c(1,0)) drymatter_TGT_particle_size_plot + drymatter_TGT_speed_plot + speed_particle_size_plot The approach here goes much beyond the interaction.plot function from the {stats} package introduced before and the code to obtain this plots is significantly longer. She has chosen to develop here the plots with {ggplot2} because she wanted to have direct access to all the minor details in the construction of the plot such as the colors by line, a custom error bars calculation, very specific locations for the legends. She ends up concluding that there is no interaction between any of the different factors as all lines do not intercept, are mostly parallel and error bars cover each other. Formula expansion f1 &lt;- Y ~ A * B * C f2 &lt;- Y ~ A * B + C expand_formula(f1) [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;A:B&quot; &quot;A:C&quot; &quot;B:C&quot; &quot;A:B:C&quot; expand_formula(f2) [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;A:B&quot; The short code chunk before shows two formula expansion examples, the first one corresponding to our Juice DOE: in a design with factors coded A, B and C the sources of variation for the Anova table for three-factor fixed effects model are: A, B, C, AB, AC, BC, ABC. The second case corresponds to a situation where interactions with C would be discarded. Understanding these syntax details is very important because as more and more factors are added to models, the number of trials grows to unrealistic quantities. In such situations a preliminary work consisting in the selection of specific interactions enables the creation a fractional design. For now the juice doe is still small with 108 trials so she can move ahead assessing the effect significance of the different factors using the anova. Anova with 3rd level interactions juice_drymatter_aov &lt;- aov( bias ~ drymatter_TGT * speed * particle_size, data = juice_drymatter) summary(juice_drymatter_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) drymatter_TGT 1 1.315 1.315 486.06 &lt;2e-16 *** speed 1 0.000 0.000 0.00 0.99 particle_size 1 0.624 0.624 230.70 &lt;2e-16 *** drymatter_TGT:speed 1 0.001 0.001 0.27 0.60 drymatter_TGT:particle_size 1 0.003 0.003 1.04 0.31 speed:particle_size 1 0.003 0.003 1.19 0.28 drymatter_TGT:speed:particle_size 1 0.004 0.004 1.44 0.23 Residuals 100 0.271 0.003 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here she simplified things by inputting the formula directly in the aov function without passing by the formula() or lm() functions. The previous observations done on the plots are fully confirmed and now supported with statistical evidence: drymatter target and particle_size significantly affect the bias (p &lt; 0.05); speed has no effect; none of the interactions is significant. This first round of assessment was very clear and successful and she can make bold proposals to the Quality Manager now to look deeper into the links between drymatter target and particle size in that bias. Certainly passing by a discussion again with the product team and a final DOE with more levels to identify or select an optimal operating zone for the measurement method. Two level designs We may be armed with powerful tools to design and analyze experiments and even have strong knowledge in the topic we’re studying but real life in a laboratory or in a factory has many constraints and a DOE is always the reflection of them. The calculation of the number of trials presented in the previous case shows a very quick explosion of the volume of work and material consumption. Another aspect is that as knowledge progresses and findings are accumulated certain variables which present little influence in the outputs start to be discarded. This is a consequence of the sparsity of effects principle. Data and models constructed in several preliminary DOEs can be consolidated under certain conditions. So the design of a new DOE should take into account the design of the previous one and this regarding not only the variables but even the levels themselves. With all these practical considerations in mind it is possible and common to start with very large screening experiments with for instance 10 inputs and 10 outputs and end up with a narrow optimization experiment with 2 factors with 4 levels to select a fine operating window. A way to make screening experiments realistic is to limit the number of levels of the factors, the minimum being 2 to have a complete factorial design. Following the notation also presented in the previous case study these designs are called \\(2^{k}\\) designs. Application of linear models and interpretation of anova is subject to the same assumptions as general cases discussed, these being the factors are fixed, the designs are completely randomized, the normality assumptions are satisfied. In particular as there are only 2 levels it is assumed that the response is approximately linear between the factor levels. In the next case studies we continue follow the same general steps: Identify factors Estimate factor effects Form initial full model Check model including residuals Assess significance of effects including factor interactions Interpret results Refine model by removing the non significant effects Re-check model Draw final conclusions In this first Case Study dedicated to \\(2^k\\) designs we’re going to start by explore the contrasts settings in the linear model functions as the coding of factors becomes a key tool in the linear model construction in R and in the way to use the forecasting tools. Case study: PET clothing improvement plan Consumer demand for recycled materials increases requiring clothing manufacturers to develop new products made with innovative and often more expensive raw materials while keeping historical quality levels. Factorial design 2 levels A materials engineer working in the winter sports clothing industry has been working in the development of a recyclable PET. Previous tests have shown promising results on tensile strength, one of the main characteristics required from the raw material. The trade offs between performance, costs and recyclability are not obvious to obtain due to lack of experience and specific know-how. Several one at a time comparisons between supplier deliveries have been done but now she wanted to go further and has established together with the raw material supplier factorial design with two factors presented in the output of the next R chunk. Most of the time process recipes at raw material producer need to are kept confidential for competitive reasons. This makes she only had access to a generic description of the factor levels: A: bi-axial orientation in production (yes/no) B: nucleating agent level (high/low) library(DoE.base) pet_doe &lt;- fac.design( randomize = FALSE, factor.names = list( A = c(&quot;-&quot;, &quot;+&quot;), B = c(&quot;-&quot;, &quot;+&quot;), replicate = c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;) ) ) After a quick check the plan is confirmed to be ok, she sees all combinations of factors at with 3 replicates. She’s not so comfortable with such a small number of replicates but as there is no prototyping tool in the producers plant they used directly an industrial laminator. Fitting trials in production time is most of the time a big challenge not to mention the cost and the waste in materials. She shares the plan in a meeting and a few weeks later receives the numbers from the producers laboratory in a short e-mail with a list of numbers with no units 64.4, 82.8, 41.4… Getting back to her contact at the producer she gets a confirmation these are the PET tensile strength values for each of the trials in the same order as the trial plan was provided. She regrets not having given a number to each trial and asked to have a clear reference of each measured value. She again compromises and collates the values to the original tibble in R: tensile_strength &lt;- c( 64.4,82.8,41.4,71.3,57.5,73.6,43.7,69.0,62.1,73.6,52.9,66.7 ) pet_doe &lt;- bind_cols( pet_doe, &quot;tensile_strength&quot; = tensile_strength, ) pet_doe %&gt;% head() %&gt;% kable() A B replicate tensile_strength - - I 64.4 + - I 82.8 - + I 41.4 + + I 71.3 - - II 57.5 + - II 73.6 Now she’s ready to move ahead by coding properly the factors and input them in the linear model. She’s not so used to DOEs with coded factors so she tries three different approaches: a first one with the factors labeled plus/minus, a second one with the factors labeled +1/-1 and a third one with the factors as +1/-1 but numeric. She ends up choosing this last option which seems more natural for forecasting. Coding levels Factors as +/- pet_plusminus &lt;- pet_doe pet_plusminus$A &lt;- relevel(pet_plusminus$A, ref = &quot;+&quot;) pet_plusminus$B &lt;- relevel(pet_plusminus$B, ref = &quot;+&quot;) For the first model the materials engineer made a copy of the original dataset and left the input variables as they were generated which is as factors and with the labels “plus” and “minus.” After some playing with data she found necessary to put the “plus” as the reference otherwise she gets inverted signs in the lm output. Another detail she needed to take care was the setup of the contrasts. As the design is orthogonal and she wanted the contrasts to add up to zero she had to precise by assigning contr.sum to the factor. First she checked the original definition of the contrasts: contrasts(pet_plusminus$A) - + 0 - 1 The original/default setting is contr.treatm as seen in the corresponding unit and she changed this with: contrasts(pet_plusminus$A) &lt;- &quot;contr.sum&quot; contrasts(pet_plusminus$B) &lt;- &quot;contr.sum&quot; contrasts(pet_plusminus$A) [,1] + 1 - -1 contrasts(pet_plusminus$B) [,1] + 1 - -1 Having confirmed that the sum of the contrast is zero she establishes the linear model and makes a prediction to check the output: pet_plusminus_lm &lt;- lm( formula = tensile_strength ~ A * B, data = pet_plusminus ) summary(pet_plusminus_lm) Call: lm.default(formula = tensile_strength ~ A * B, data = pet_plusminus) Residuals: Min 1Q Median 3Q Max -4.60 -3.07 -1.15 2.49 6.90 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 63.25 1.31 48.14 3.8e-11 *** A1 9.58 1.31 7.29 8.4e-05 *** B1 -5.75 1.31 -4.38 0.0024 ** A1:B1 1.92 1.31 1.46 0.1828 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.55 on 8 degrees of freedom Multiple R-squared: 0.903, Adjusted R-squared: 0.867 F-statistic: 24.8 on 3 and 8 DF, p-value: 0.000209 predict(pet_plusminus_lm, newdata = list(A = &quot;+&quot;, B = &quot;+&quot;)) 1 69 Factors as +/- 1 coded &lt;- function(x) { ifelse(x == x[1], -1, 1) } pet_doe &lt;- pet_doe %&gt;% mutate(cA = coded(A), cB = coded(B)) pet_plusminus1 &lt;- pet_doe %&gt;% mutate(across(c(cA, cB), as_factor)) pet_plusminus1$cA &lt;- relevel(pet_plusminus1$cA, ref = &quot;1&quot;) pet_plusminus1$cB &lt;- relevel(pet_plusminus1$cB, ref = &quot;1&quot;) pet_plusminus1 %&gt;% head(3) %&gt;% kable(align = &quot;c&quot;) A B replicate tensile_strength cA cB - - I 64.4 -1 -1 + - I 82.8 1 -1 - + I 41.4 -1 1 The second approach she tries is to convert the levels to +1/-1 still leaving them coded as factors. This notation is easier for her as it corresponds to a common way she sees in the Yates tables. Again she had to relevel the factors to get the max as reference in order to get the same coefficients on the linear model. Regarding the contrasts she goes for the simpler and more direct approach now by defining them directly in the lm() function. pet_plusminus1_lm &lt;- lm( formula = tensile_strength ~ cA * cB, data = pet_plusminus1, contrasts = list(cA = &quot;contr.sum&quot;, cB = &quot;contr.sum&quot;) ) summary(pet_plusminus1_lm) Call: lm.default(formula = tensile_strength ~ cA * cB, data = pet_plusminus1, contrasts = list(cA = &quot;contr.sum&quot;, cB = &quot;contr.sum&quot;)) Residuals: Min 1Q Median 3Q Max -4.60 -3.07 -1.15 2.49 6.90 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 63.25 1.31 48.14 3.8e-11 *** cA1 9.58 1.31 7.29 8.4e-05 *** cB1 -5.75 1.31 -4.38 0.0024 ** cA1:cB1 1.92 1.31 1.46 0.1828 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.55 on 8 degrees of freedom Multiple R-squared: 0.903, Adjusted R-squared: 0.867 F-statistic: 24.8 on 3 and 8 DF, p-value: 0.000209 predict(pet_plusminus1_lm, newdata = list(cA = &quot;1&quot;, cB = &quot;1&quot;)) 1 69 Note that a coefficient in a regression equation is the change in the response when the corresponding variable changes by +1. Special attention to the + and - needs to be taken with the R output. As A or B changes from its high level to its low level, the coded variable changes by 1 − (−1) = +2, so the change in the response from the min to the max is twice the regression coefficient. So the effects and interaction(s) from their minimum to their maximum correspond to twice the values in the “Estimate” column. These regression coefficients are often called effects and interactions, even though they differ from the definitions used in the designs themselves. Factors as +/- 1 numeric pet_num &lt;- pet_doe %&gt;% mutate(cA = coded(A), cB = coded(B)) pet_num_lm &lt;- lm( formula = tensile_strength ~ cA * cB, data = pet_num ) summary(pet_num_lm) Call: lm.default(formula = tensile_strength ~ cA * cB, data = pet_num) Residuals: Min 1Q Median 3Q Max -4.60 -3.07 -1.15 2.49 6.90 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 63.25 1.31 48.14 3.8e-11 *** cA 9.58 1.31 7.29 8.4e-05 *** cB -5.75 1.31 -4.38 0.0024 ** cA:cB 1.92 1.31 1.46 0.1828 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.55 on 8 degrees of freedom Multiple R-squared: 0.903, Adjusted R-squared: 0.867 F-statistic: 24.8 on 3 and 8 DF, p-value: 0.000209 predict(pet_num_lm, newdata = list(cA = 1, cB = 1)) 1 69 Finally the materials engineer coded the levels with +1/-1 but left the variables with type numeric. In this case she did not define any contrasts. Looking into the lm and prediction she confirms having obtained exactly the same outputs. As the inputs are coded as numeric this behaves just like the predictions with the first linear model studied in our book. Note that we feed the predictions function with numeric values. This is very intuitive as it corresponds to the original units of the experiments (also called natural or engineering units). On the other hand coding the design variables provides another advantage: generally, the engineering units are not directly comparable while coded variables are very effective for determining the relative size of factor effects. Coding the design factors has the benefit of enabling a direct comparison of the effect sizes and we can see that these three ways of coding the variable levels lead to equivalent results both in lm and prediction. Her preference goes to using numeric values as it is more intuitive and allows for easier prediction between the fixed levels. In order to better visualize the coding of factors she established a simple regression plot of the data. Note that she had to extract the data from the S3 doe object, which we’ve done with using unclass() and then as_tibble() pet_num %&gt;% unclass() %&gt;% as_tibble() %&gt;% mutate(cA = coded(A), cB = coded(B)) %&gt;% pivot_longer( cols = c(&quot;cA&quot;, &quot;cB&quot;), names_to = &quot;variable&quot;, values_to = &quot;level&quot;) %&gt;% ggplot() + geom_point(aes(x = level, y = tensile_strength)) + geom_smooth(aes(x = level, y = tensile_strength), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + coord_cartesian(xlim = c(-1.5, 1.5), ylim = c(30, 90)) + scale_y_continuous(n.breaks = 10) + facet_wrap(vars(variable)) + labs( title = &quot;PET tensile strength improvement DOE&quot;, y = &quot;Tensile strength [MPa]&quot;, x = &quot;Coded levels&quot; ) From the lm() summary she remembers that the intercept passes at 27.5 and she reports now to putting the B factor at its maximum: pet_num %&gt;% unclass() %&gt;% as_tibble() %&gt;% mutate(cA = coded(A), cB = coded(B)) %&gt;% filter(cB == 1) %&gt;% pivot_longer( cols = c(&quot;cA&quot;, &quot;cB&quot;), names_to = &quot;variable&quot;, values_to = &quot;level&quot;) %&gt;% ggplot() + geom_point(aes(x = level, y = tensile_strength)) + geom_smooth(aes(x = level, y = tensile_strength), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE) + coord_cartesian(xlim = c(-1.5, 1.5), ylim = c(30, 90)) + scale_y_continuous(n.breaks = 10) + facet_wrap(vars(variable)) + labs( title = &quot;PET tensile strength improvement DOE&quot;, y = &quot;Tensile strength [MPa]&quot;, x = &quot;Coded levels&quot; ) The plot confirms that the output of the prediction is 69 corresponding to the max level of A when B is also at the max. Mathematically she confirms this result by multiplying all the linear regression coefficients by the levels of the factors as : \\(63.250 + 9.583 \\times (+1) - 5.750 \\times (+1) + 1.917 = 69\\) Interaction plots with SE library(RcmdrMisc) par(mfrow = c(1,1), bty = &quot;l&quot;) plotMeans(response = pet_doe$tensile_strength, factor1 = pet_doe$A, xlab = &quot;A: bi-axial orientation in production (yes/no)&quot;, factor2 = pet_doe$B, legend.lab = &quot;B: nucleating agent (high/low)&quot;, ylab = &quot;Tensile Strenght [Mpa]&quot;, error.bars = &quot;se&quot;, col = viridis::viridis(12)[4], legend.pos = &quot;bottomright&quot;, main = &quot;The PET clothing improvement plan&quot;) dev.off() null device 1 Now she wanted to get quickly an interaction plot but including error bars. Unfortunately the base R interaction.plot() doesn’t provide it and the ggplot2() made it to long. With a quick check on Stackoverflow she discovered this simple approach with the function plotMeans() from the package {RcmdrMisc} and she gets the plot dine with standard error as argument for the error.bars argument. As expected she confirms that both treatments provide an visible effect on Tensile strength and that there is no interaction between them. Relative effects plot A final interesting analysis is the comparison of the effects in relative terms. This is common for main effects and is shortly presented in the plot below: intercept &lt;- pet_num_lm$coefficients[1] pet_new_A &lt;- list(cA = c(-1,1), cB = c(0,0)) pet_predict_A &lt;- predict(pet_num_lm, newdata = pet_new_A) pet_new_B &lt;- list(cA = c(0,0), cB = c(-1,1)) pet_predict_B &lt;- predict(pet_num_lm, newdata = pet_new_B) pet_effects &lt;- tibble( level = c(-1, 1), A = pet_predict_A, B = pet_predict_B ) %&gt;% pivot_longer( cols = c(A, B), values_to = &quot;tensile_strength&quot;, names_to = &quot;factor&quot; ) %&gt;% mutate( factor_level = str_c(factor, &quot; &quot;, level), tensile_strength_variation = tensile_strength - intercept ) pet_effects %&gt;% ggplot( aes(x = tensile_strength_variation, y = factor_level, fill = factor) ) + geom_col(position = &quot;dodge2&quot;) + scale_x_continuous(n.breaks = 10) + labs( title = &quot;Relative effects plot&quot;, subtitle = &quot;Tensile strength variation for factor extremes&quot;, y = &quot;Factor / Level&quot;, x = &quot;Tensile strength variation&quot; ) The plot is complementary to the previous analysis and helps us quickly grasp that the factor A (bi-axial orientation in production has a stronger effect than the factor B (nucleating agent level). Adjusted R-squared Case study: lithium-ion battery charging time The global transition to full electrical car is well underway and there’s a global trend in legislating towards the end of fossil fuel cars. The challenge of autonomy has been brought to acceptable levels with the extensive deployment of electric charging stations but engineering teams still face complex problems such as the charging time. At a pioneering manufacturer another DOE is underway to get it optimized. battery_lm &lt;- lm( formula = charging_time ~ A * B * C, data = battery_charging ) summary(battery_lm) Call: lm.default(formula = charging_time ~ A * B * C, data = battery_charging) Residuals: Min 1Q Median 3Q Max -2.595 -1.076 -0.450 0.965 4.155 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.787 0.325 20.89 &lt;2e-16 *** A 0.940 0.325 2.89 0.0080 ** B -0.182 0.325 -0.56 0.5813 C 1.040 0.325 3.20 0.0038 ** A:B 0.163 0.325 0.50 0.6208 A:C -0.809 0.325 -2.49 0.0201 * B:C -0.349 0.325 -1.07 0.2932 A:B:C 0.408 0.325 1.26 0.2214 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.84 on 24 degrees of freedom Multiple R-squared: 0.54, Adjusted R-squared: 0.405 F-statistic: 4.02 on 7 and 24 DF, p-value: 0.00481 The R-squared was introduced in the linear models unit as a way to assess the quality of the model fit. A potential problem with this statistic is that it always increases as factors are added to the model, even if these factors are not significant. This can be overcome by using the adjusted R-squared which is obtained by dividing the Sums of Squares by the degrees of freedom, and is adjusted for the size of the model, that is the number of factors. Both indicators are part of the summary() output of the lm() function applied on the charging_time dataset as we could just see in the previous chunk. Below we’re comparing both indicators. A consulting company specialized in data science is supporting a global manufacturer of electrical car batteries to further optimize a lithium-ion battery charging time. The latest DOE consisted of 3 input factors as follows: A - temperature (-1 = -10°C, +1 = 40°C) B - voltage (-1 = 120V, +1 = 220V) C - age (-1 = 10’000 cycles, +1 = 0 cycles) Z - charging time [h] The model can now be passed to the aov() function for an assessment of the significance of the different factors: battery_aov &lt;- aov(battery_lm) summary(battery_aov) Df Sum Sq Mean Sq F value Pr(&gt;F) A 1 28.3 28.3 8.37 0.0080 ** B 1 1.1 1.1 0.31 0.5813 C 1 34.6 34.6 10.26 0.0038 ** A:B 1 0.8 0.8 0.25 0.6208 A:C 1 20.9 20.9 6.20 0.0201 * B:C 1 3.9 3.9 1.15 0.2932 A:B:C 1 5.3 5.3 1.58 0.2214 Residuals 24 81.0 3.4 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The main effects of temperature and age are significant as is their interaction. Voltage has no influence on the output. An updated model is prepared considering these observations and removing the factor B. battery_reduced_lm &lt;- lm( formula = charging_time ~ A * C, data = battery_charging ) summary(battery_reduced_lm) Call: lm.default(formula = charging_time ~ A * C, data = battery_charging) Residuals: Min 1Q Median 3Q Max -3.696 -1.062 -0.483 0.952 3.054 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.787 0.321 21.16 &lt;2e-16 *** A 0.940 0.321 2.93 0.0067 ** C 1.040 0.321 3.24 0.0030 ** A:C -0.809 0.321 -2.52 0.0176 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.81 on 28 degrees of freedom Multiple R-squared: 0.476, Adjusted R-squared: 0.42 F-statistic: 8.49 on 3 and 28 DF, p-value: 0.000361 Besides the base summary() function, R squared and adjusted R squared can also be easily retrieved with the glance function from the {broom} package. We’re extracting them here for the complete and for reduced model: glance(battery_lm)[1:2] %&gt;% bind_rows(glance(battery_reduced_lm)[1:2], .id = &quot;model&quot;) # A tibble: 2 x 3 model r.squared adj.r.squared &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.540 0.405 2 2 0.476 0.420 Although R-squared has decreased the adjusted R-squared has slightly improved showing that removing the non significant terms has resulted in a better fit. The changes are small and further work is still required but the principle is clear that the model fit is improving and will better forecast the output for new data. Coding inputs natural2coded &lt;- function(xA, lA, hA) {(xA - (lA + hA) / 2) / ((hA - lA) / 2)} # Converting natural value xA into coded value cA: lA &lt;- -10 hA &lt;- 40 nA &lt;- 22 cA &lt;- natural2coded(nA, lA, hA) cA [1] 0.28 The consulting company proposed itself to provide a tool to predict the charging time for new batteries. They had been doing lots of the DOEs and based on the last one they’ve realized they believe to be in a position to calculate the response at a certainly specific level between the coded factor levels of \\(\\pm\\) 1. To do that they needed to convert natural values into coded values. They’ve given as an example a calculation of the charging time for a temperature of which the natural value is nA = 22 [°C] which is between the natural levels of lA = -10 and hA = 40 [°C]. This temperature coded corresponds to a value of 0.28 °C as presented in the previous chunk. To be noted that the opposite conversion would look like: coded2natural &lt;- function(cA, lA, hA) {cA * ((hA - lA) / 2) + ((lA + hA)/2)} # Converting back the coded value cA into its natural value xA lA &lt;- -10 hA &lt;- 40 nA &lt;- coded2natural(cA, lA, hA) nA [1] 22 Coded prediction battery_new &lt;- tibble(A = cA, C = 1) pA &lt;- predict(battery_reduced_lm, battery_new) pA 1 7.8634 They’ve chosen to do this prediction for a fixed level of C of 1, corresponding to a completely new battery (maximum of the factor C at of 0 cycles) and the predict() function with those values and the reduced model. We can visualize the outcome as follows: battery_charging %&gt;% filter(C == 1) %&gt;% ggplot() + # geom_point(aes(x = A, y = charging_time, color = as_factor(C))) + geom_smooth(aes(x = A, y = charging_time), method = &quot;lm&quot;, se = FALSE) + geom_point(aes(x = cA, y = pA)) + scale_y_continuous(n.breaks = 10) + scale_color_discrete(guide = FALSE) + theme(plot.title = ggtext::element_markdown()) + geom_hline(yintercept = pA, linetype = 2) + coord_cartesian(xlim = c(-1, 1)) + scale_x_continuous(n.breaks = 5) + scale_y_continuous(n.breaks = 20) + labs( title = &quot;Lithium-ion battery charging DOE&quot;, y = &quot;Charging time [h]&quot;, x = &quot;A: temperature (-1 = -10°C, +1 = 40°C)&quot;, subtitle = &quot;Prediction with reduced model&quot;) Perspective plot library(rsm) persp( battery_reduced_lm, A ~ C, bounds = list(A = c(-1,1), C = c(-1,1)), col = viridis(12)[8], theta = -40, phi = 20, r = 5, zlab = &quot;Charging Time [h]&quot;, xlabs = c( &quot;A: Temperature&quot;, &quot;C: Age&quot;), main = &quot;Lithium-ion battery\\ncharging time DOE&quot; ) Here they went further introducing here response surface plots which is yet another way to visualize the experiment outputs as a function of the inputs. They’ve done this with the persp() function from the {rsm} package which provides an extremely fast rendering, easy parametrization and a readable output. To be noted that this function is an extension of the base R persp() consisting from the R point of view in an S3 method for the lm class. This allows to simply provide directly the lm object to the function to obtain the response surface. Due to the interaction between factors A and C the surface is bent. This is exactly what we observe in the interactions plots of which the one below corresponds to slicing the surface at the min and the max of Power: interaction.plot(x.factor = battery_charging$C, trace.factor = battery_charging$A, fun = mean, response = battery_charging$charging_time, legend = TRUE, xlab = &quot;C: Age \\n(-1 = 10&#39;000 cycles, +1 = 0 cycles)&quot;, trace.label = &quot;A: Temperature \\n(+1 = 40°C, -1 = -10°C)&quot;, lwd = 2, lty = c(2,1), col = viridis(12)[8], ylab = &quot;Charging Time&quot;, main = &quot;Lithium-ion battery\\ncharging time test&quot;) Just like in the surface plot we can see here in the interaction plot that the response of charging time on age is very different depending on the level of temperature. When temperature is at its max the charging time is almost independent of age but at the minimum of temperature the charging time depends a lot on the age. All this make a lot of sense to everyone involved but its good to confirm it with results and to get the details of how much these variations are numerically. As a reminder this is what is called an interaction between these two factors. Single replicate battery_sr_lm &lt;- lm( formula = charging_time ~ A * B * C * D, data = battery_charging %&gt;% filter(Replicate == 2)) summary(battery_sr_lm) Call: lm.default(formula = charging_time ~ A * B * C * D, data = battery_charging %&gt;% filter(Replicate == 2)) Residuals: ALL 16 residuals are 0: no residual degrees of freedom! Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.44e+00 NaN NaN NaN A 1.76e+00 NaN NaN NaN B -2.50e-02 NaN NaN NaN C 6.75e-01 NaN NaN NaN D 1.05e+00 NaN NaN NaN A:B 7.50e-02 NaN NaN NaN A:C -9.75e-01 NaN NaN NaN B:C -3.12e-01 NaN NaN NaN A:D 4.00e-01 NaN NaN NaN B:D 4.13e-01 NaN NaN NaN C:D 1.25e-02 NaN NaN NaN A:B:C 4.12e-01 NaN NaN NaN A:B:D -1.13e-01 NaN NaN NaN A:C:D -2.63e-01 NaN NaN NaN B:C:D 5.00e-02 NaN NaN NaN A:B:C:D 6.94e-18 NaN NaN NaN Residual standard error: NaN on 0 degrees of freedom Multiple R-squared: 1, Adjusted R-squared: NaN F-statistic: NaN on 15 and 0 DF, p-value: NA In the R&amp;D offices of the manufacturer of electrical car batteries there is some satisfaction with the report delivered by the data science consulting company. Although initially skeptical the head of battery engineering has finally acknowledged that there are several benefits coming from this work. Now, he makes a last moment request: he would like to know by Thursday (after tomorrow) what is the effect of the terminals material. In his view this will for sure have a high impact on the final delivered cost of the assembled battery. Unfortunately data on terminals material was only captured in the 2nd replicate. We can check that in the original data for example in first two and last two rows. The variable coded as D is missing in the beginning: battery_charging[c(1,2,31:32),] # A tibble: 4 x 6 A B C D Replicate charging_time &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 -1 -1 -1 -1 NA 3.5 2 1 -1 -1 -1 NA 6.69 3 -1 1 1 1 2 7 4 1 1 1 1 2 9.6 As a reminder below the DOE variables, including D are: A - temperature (-1 = -10°C, +1 = 40°C) B - voltage (-1 = 120V, +1 = 220V) C - age (-1 = 10’000 cycles, +1 = 0 cycles) D - terminal (-1 = lead based, +1 = zinc based) Z - charging time [h] As there is no time to collect new data, a specialist in DOEs from the consulting company suggests exploiting the the single replicate data using a graphical method - the normal probability plot - to identify the main effects that are important in the model. He demonstrates how to achieve this with the function qqPlot() from the {car} package: Effects normal plot library(car) battery_sr_effects &lt;- battery_sr_lm$coefficients[2:16] battery_sr_effects_names &lt;- names((battery_sr_lm$coefficients)[2:16]) main_effects_plot &lt;- qqPlot( ylab = &quot;Model effects&quot;, battery_sr_effects, envelope = 0.95, id = list( method = &quot;y&quot;, n = 5, cex = 1, col = carPalette()[1], location = &quot;lr&quot;), grid = FALSE, col = &quot;black&quot;, col.lines = viridis::viridis(12)[5], main = &quot;Battery charging DOE\\nNormal plot of effects&quot; ) In plot we can see that the effects that have the highest influence on the output are the effects A - temperature and D - terminal and their interaction. Its seems the head of engineering had a good intuition. The next step is a confirmation of these observations with a calculation of the percentage contribution of each effect as follows: Effects contribution table battery_sr_lm_tidy &lt;- battery_sr_lm %&gt;% tidy() %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% mutate( effect_estimate = 2 * estimate) battery_sr_aov &lt;- aov(battery_sr_lm) battery_sr_aov_tidy &lt;- battery_sr_aov %&gt;% tidy() %&gt;% mutate(term, sumsq_total = sum(sumsq), effect_contribution_perc = sumsq/sumsq_total*100) main_effects_table &lt;- battery_sr_lm_tidy %&gt;% left_join(battery_sr_aov_tidy, by = &quot;term&quot;) %&gt;% select(term, estimate, effect_estimate, sumsq, effect_contribution_perc) %&gt;% arrange(desc(effect_contribution_perc)) main_effects_table %&gt;% head(5) %&gt;% kable() term estimate effect_estimate sumsq effect_contribution_perc A 1.7625 3.525 49.7025 49.2799 D 1.0500 2.100 17.6400 17.4900 A:C -0.9750 -1.950 15.2100 15.0807 C 0.6750 1.350 7.2900 7.2280 B:D 0.4125 0.825 2.7225 2.6993 We could see in the lm() output before that no statistics have been calculated for the effects in the model as there is only a single replicate. Reduced model battery_red_lm &lt;- lm( formula = charging_time ~ A + D + A:C, data = battery_charging %&gt;% filter(Replicate == 2)) summary(battery_red_lm) Call: lm.default(formula = charging_time ~ A + D + A:C, data = battery_charging %&gt;% filter(Replicate == 2)) Residuals: Min 1Q Median 3Q Max -2.450 -0.338 0.163 0.425 2.200 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.438 0.309 20.85 8.6e-11 *** A 1.763 0.309 5.71 9.8e-05 *** D 1.050 0.309 3.40 0.0053 ** A:C -0.975 0.309 -3.16 0.0083 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.24 on 12 degrees of freedom Multiple R-squared: 0.819, Adjusted R-squared: 0.773 F-statistic: 18 on 3 and 12 DF, p-value: 9.63e-05 Following theses analysis a new model has been built, including only the effects and interactions with highest contribution. We can now see that we’ve regained degrees of freedom and obtained a sort of hidden replication allowing to calculate statistics and error terms on the model. Checking the residuals the DOE specialist from the consulting company recommends to do another test now with proper replication but choosing only the variables of interest. These residuals show the limitations of this model deviating from normality above \\(\\pm\\) 1 standard deviation and showing difference variance at different levels. par(mfrow = c(2,3)) plot(battery_red_lm$residuals) plot(battery_red_lm, which = 2) plot(battery_red_lm, which = c(1, 3, 5)) plot(battery_red_lm, which = 4) dev.off() null device 1 In any case from the linear model coefficients we can already see that the selection of terminal material has a significant effect which is of about 60% of the effect of the temperature (1’050/1’763). References "],["SPC.html", "Statistical Process Control xbar-R charts Cpk charts I-MR charts", " Statistical Process Control Keeping the variability of an industrial process under control is one of the most important objectives in manufacturing. Based on expert knowledge or on detailed functional analysis the product and process parameters that are critical to quality are identified and selected for close follow-up. The most common and effective way for such follow-up is the Statistical Process Control which is done by using control charts. Case study: Syringe injection molding Pharmaceutical plastic injection is always a large scale manufacturing process with high speeds and tight specifications. Significant investments in clean rooms, fast automation and skilled operators require a commitment to the long term quality and a culture of excellence. xbar-R charts There are many types of control charts and in this case study we’re demonstrating the xbar and R charts. These two charts are often used together and are suited to the control the mean and the variability of a continuous variable. This case study draws on examples from Bass (2007) and from the {qcc} package vignette. PH-parts is a plastic injection company specialized in high precision pharmaceutical Parts. In their catalog they have a mid-range product that sells in extremely high volumes: vaccination syringes. The inner diameter of the barrel has been identified in failure mode analysis as a critical dimension for the security of the operation. Following the implementation of a statistical process control (SPC) the production operators monitor the production process using xbar and R-charts. The protocol requires 6 samples are taken on an hourly basis and diameters simply typed in an excel file in a laptop by the injection press. A recent excel file with samples taken over a period of 25 hours of production has been loaded in R with the name syringe_diameter by the Quality Assistant of the lab. head(syringe_diameter) %&gt;% kable() Hour Sample1 Sample2 Sample3 Sample4 Sample5 Sample6 Hour1 5.3314 5.3399 5.3244 5.3363 5.3228 5.3181 Hour2 5.3240 5.3214 5.3142 5.3237 5.3420 5.3392 Hour3 5.3263 5.3404 5.3136 5.3565 5.3387 5.3570 Hour4 5.3553 5.3600 5.3171 5.3319 5.3446 5.3474 Hour5 5.3379 5.3264 5.3150 5.3134 5.3375 5.3407 Hour6 5.3432 5.3352 5.3238 5.3463 5.3340 5.3205 In this table each line corresponds to a sampling hour and each column corresponds to a sample number. We’re now going to pass this data to the control chart plotting function qcc() from the package with the same name. We choose to copy the dataset to a new one to remove the Hour column with the select function from tidyverse and we take the opportunity to round the values: syringe_clean &lt;- syringe_diameter %&gt;% select(-Hour) %&gt;% mutate(across(starts_with(&quot;S&quot;), round, 2)) In order to establish a control chart it is recommended to run a “calibration run.” The calibration run is used to calculate the control limits before entering “regular production.” Using the first 10 samples we call the qcc() function to make the required calculations. Mean chart library(qcc) syringe_xbar &lt;- qcc( syringe_clean[1:10, ], type = &quot;xbar&quot;, title = &quot;Syringe injection molding\\n barrel diameter xbar chart (calibration)&quot;, xlab = &quot;Sample group&quot;, plot = FALSE ) In the previous chink we’ve loaded the {qcc} package that has the required quality control tools and created a new variable data but before we plot the chart lets look a bit in detail in the calculations done in the background. A first step is to read the beginning of the qcc() help file typing ?qcc in the console. It says \"Create an object of class ‘qcc’ to perform statistical process control’ (in R technical terms function is a helper that generates an S3 R object). The key point here is that this means we can inspect the calculations separately from the plot itself. We can start by confirming the class and the type of the qcc object: class(syringe_xbar) [1] &quot;qcc&quot; typeof(syringe_xbar) [1] &quot;list&quot; It is confirmed it is an object of class qcc with the type list. Looking into the structure of the list: str(syringe_xbar) List of 11 $ call : language qcc(data = syringe_clean[1:10, ], type = &quot;xbar&quot;, plot = FALSE, title = &quot;Syringe injection molding\\n barrel diamet| __truncated__ $ type : chr &quot;xbar&quot; $ data.name : chr &quot;syringe_clean[1:10, ]&quot; $ data : num [1:10, 1:6] 5.33 5.32 5.33 5.36 5.34 5.34 5.3 5.32 5.34 5.36 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ Group : chr [1:10] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .. ..$ Samples: chr [1:6] &quot;Sample1&quot; &quot;Sample2&quot; &quot;Sample3&quot; &quot;Sample4&quot; ... $ statistics: Named num [1:10] 5.33 5.33 5.34 5.34 5.33 ... ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ sizes : int [1:10] 6 6 6 6 6 6 6 6 6 6 $ center : num 5.33 $ std.dev : num 0.0142 $ nsigmas : num 3 $ limits : num [1, 1:2] 5.32 5.35 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr &quot;&quot; .. ..$ : chr [1:2] &quot;LCL&quot; &quot;UCL&quot; $ violations:List of 2 ..$ beyond.limits : int(0) ..$ violating.runs: num(0) - attr(*, &quot;class&quot;)= chr &quot;qcc&quot; The output is not easy to read but we present it here just to show that inside the list there are several tables with the statistical analysis required for our plot. If we want like to see for instance the standard deviation we can extract it separately: syringe_xbar$std.dev [1] 0.014207 And if we want like to see a summary of all the data stored in the object we could apply the summary method: summary(syringe_xbar) Call: qcc(data = syringe_clean[1:10, ], type = &quot;xbar&quot;, plot = FALSE, title = &quot;Syringe injection molding\\n barrel diameter xbar chart (calibration)&quot;, xlab = &quot;Sample group&quot;) xbar chart for syringe_clean[1:10, ] Summary of group statistics: Min. 1st Qu. Median Mean 3rd Qu. Max. 5.3250 5.3283 5.3333 5.3333 5.3375 5.3433 Group sample size: 6 Number of groups: 10 Center of group statistics: 5.3333 Standard deviation: 0.014207 Control limits: LCL UCL 5.3159 5.3507 We are now ready to finally we can see this all together in a plot: plot(syringe_xbar) Range chart syringe_R &lt;- qcc( syringe_clean[1:10, ], type = &quot;R&quot;, title = &quot;Syringe injection molding\\n barrel diameter R chart (calibration)&quot;, xlab = &quot;Sample group&quot; ) Using the same 10 first samples we also obtained the corresponding R chart. Now that the calibration data has been plotted we can consider that the control limits are defined. They can become fixed and reused in new plots for the future production runs. Samples from those future runs can then be assessed against this limits and the control chart rules can be verified (in this example the shewhart rules are used). We now add the remaining data points to our chart by specifying which lines we’re referring too in our dataframe in the ‘newdata’ argument: syringe_xbar &lt;- qcc( data = syringe_clean[1:10, ], newdata = syringe_clean[11:25,], type = &quot;xbar&quot;, title = &quot;Syringe injection molding\\n barrel diameter xbar chart (calibration)&quot;, xlab = &quot;Sample group&quot; ) We can see that the data point corresponding to the average of the measurements of the sample group 17 is plotted in red because it is outside of the control limits. Now we plot the R chart to assess the variability: syringe_R &lt;- qcc( data = syringe_clean[1:10, ], newdata = syringe_clean[11:25,], type = &quot;R&quot;, title = &quot;Syringe injection molding\\n barrel diameter R chart (calibration)&quot;, xlab = &quot;Sample group&quot; ) In this case all the points are within the previously defined control limits. More tight controls can be put in place by clearly identifying warning limits in a narrower range than the control limits. These measures need to be accompanied by clear decision criteria and proper training to avoid the typical problem of overreacting and destabilizing the process by introducing unintended special causes of variation. Warning limits warn.limits &lt;- limits.xbar( syringe_xbar$center, syringe_xbar$std.dev, syringe_xbar$sizes, 2 ) plot( syringe_xbar, restore.par = FALSE, title = &quot;Syringe injection molding\\n barrel diameter xbar chart&quot;, xlab = &quot;Sample group&quot;) abline(h = warn.limits, lty = 3, col = viridis(12)[1]) The previous chunk exemplified how to add warning limits in the xbar chart and now we’re going to look into specifications. A manufacturing process under control has a variation that is lower than the product specifications and is ideally well centered. Therefore it is usually good practice to follow the control chart rules referring to the process control limits. In some cases Control Charts are even issued without the product specifications but in some cases there may an interest to add the specification limits. To to this we first we establish the specifications: spec_max &lt;- 5.6 spec_min &lt;- 5.3 spec_tgt &lt;- (spec_max - spec_min) / 2 + spec_min specs &lt;- c(spec_min, spec_tgt, spec_max) and the plot again the control chart with visible specification limits and targets: plot( syringe_xbar, restore.par = FALSE, title = &quot;Syringe injection molding\\n barrel diameter xbar chart (calibration)&quot;, xlab = &quot;Sample group&quot;, ylim = c(specs[1], specs[3]) ) abline(h = specs, lty = 3, col = &quot;red&quot;) In our case study something has gone severely off track. The barrel diameter was considered under control but now when we added the specification limits we see a situation that sometime happens in practice and that requires action. The data plotted is still within the min max specification limits for this relatively small number of data points and the variation is overall well contained within the process limits but we see it is extremely off centered when compared with the product specification. A process capability study should help determining the causes for this off centering and help correcting it. Cpk charts In order to optimize the manufacturing process, the manufacturing engineering expert is going to assess the capability of the syringe injection diameter. Such details are not straight forward to modify. Should he request a change of some injection mold parts? Or play with process temperatures? Before moving to a process DOE he needs first to understand where he stands. We’re going to go more in depth in the study of the manufacturing process and make a comparison between the product specifications and the process variability. We’re looking for opportunities to tighten the product specifications. Tightening a product specification without increasing the cost of a manufacturing cost can be a very good source of competitive advantage. The way we are going to do this is by constructing an histogram and a density distribution of this process. Off spec off_spec &lt;- function(UCL, LCL, mean, sd) { round(100 - ((stats::pnorm(UCL, mean, sd) - stats::pnorm(LCL, mean, sd))*100), 2) } This simple function off_spec() provides a probabilistic calculation of the number of parts that can be out of certain specification limits provided a certain sample mean, standard deviation and specification limits. This function is included in our package {industRial} and can accessed any time with industRial::off_spec. Now as a first step we’re going to calculate process parameters such as mean and standard deviation that we will use with the function. For this we put the data in a long format. syringe_long &lt;- syringe_diameter %&gt;% pivot_longer(cols = starts_with(&quot;Sample&quot;), names_to = &quot;sample&quot;, values_to = &quot;value&quot;) syringe_mean = syringe_long %&gt;% pull(value) %&gt;% mean() syringe_sd = syringe_long %&gt;% pull(value) %&gt;% sd() syringe_n &lt;- length(syringe_long) and obtain: syringe_off_spec &lt;- off_spec(spec_max, spec_min, syringe_mean, syringe_sd) syringe_off_spec [1] 1.59 For our histogram we also want to obtain data for the density function. This can be calculated using the function rnorm() from the {stats} package using a reasonably high sample size. Here we use 100’000. theor_n = 1000000 syringe_theor &lt;- rnorm(n = theor_n, mean = syringe_mean, sd = syringe_sd) %&gt;% as_tibble() Now we prepare the legend of the chart using the variables created: plot_subtitle &lt;- paste( &quot;Spec: [&quot;, spec_min, &quot;;&quot;, spec_max, &quot;], Expected off-spec = &quot;, signif(syringe_off_spec, digits = 2), &quot;%&quot; ) Note that we deliberately tweak the plot colors to make it look like the plots from minitab and from the {qcc} package. We provide this theme in the book companion package industRial with the name theme_qcc(). syringe_long %&gt;% ggplot(aes(x = value, y = ..density..)) + geom_histogram( bins = 30, fill = &quot;white&quot;, color = &quot;grey20&quot;) + geom_density(data = syringe_theor, linetype = 2) + geom_vline(xintercept = {spec_min}, color = &quot;red&quot;, linetype = 3) + geom_vline(xintercept = {spec_max}, color = &quot;red&quot;, linetype = 3) + geom_vline(xintercept = {spec_tgt}, color = &quot;red&quot;, linetype = 2) + scale_x_continuous(n.breaks = 10) + theme_qcc() + labs( title = &quot;Syringe barrel diameter Capability Study&quot;, subtitle = {plot_subtitle}) By looking at the histogram we confirm the extreme off centering of the production. We also see that although there are no measurements beyond the lower specification limit (LSL) in the small sample of 25 values collected initially but in a high volume production the defect rate is very high. Defect rates typical in this industry will be not in the percentage but in the range of the ppm (parts per million). Process Capability process_Cpk &lt;- function(UCL, LCL, mean, sd) { pmin( (abs(mean - abs(LCL)) / (3 * sd)), (abs((abs(UCL) - mean)) / (3 * sd)) ) } The process_Cpk() is another function included in our package with which we can re-use the process variables calculated before to estimate the Cpk index: process_Cpk(spec_max, spec_min, syringe_mean, syringe_sd) [1] 0.71587 The Quality Management department expects to see Capability Reports in ppm and not in percentage To make such conversion we have to multiply by 10’000. Note that we’re not considering the 1.5 shift that sometimes is presented in the literature but rather making a simple direct conversion of the proportion out of spec found before: formatC(((syringe_off_spec) * 10000), format = &quot;d&quot;, big.mark = &quot;&#39;&quot;) [1] &quot;15&#39;900&quot; As mentioned the expected population below the LSL is 1,3% which is very high for industry standards. In fact this corresponds to 15’900 parts per million (ppm) whereas a common target would be 1 ppm. Naturally these figures are indicative and they depend of the context criteria such as severity of the problem, cost, difficulty to eliminate the problem and so on. We are here going to establish a simple reference table using the functions created before, to present the expected percentage that falls within certain limits. To make it useful as a reference table we’re putting this limits from \\(\\pm\\) 1 to \\(\\pm\\) 6 standard deviations Sigma conversion table sigma_limits &lt;- tibble( sigma_plus = c(1, 2, 3, 4, 5, 6), sigma_minus = -sigma_plus, mean = 0, sd = 1 ) sigma_limits %&gt;% mutate( off_spec_perc = off_spec(sigma_plus, sigma_minus, mean, sd), in_spec_perc = 100 - off_spec_perc, Cpk = process_Cpk(sigma_plus, sigma_minus, mean, sd), ppm_defects = formatC( off_spec(sigma_plus, sigma_minus, mean, sd) * 10000, format = &quot;d&quot;, big.mark = &quot;&#39;&quot;)) %&gt;% select(sigma_minus, sigma_plus, off_spec_perc, in_spec_perc, Cpk, ppm_defects) %&gt;% kable(align = &quot;c&quot;, digits = 3) sigma_minus sigma_plus off_spec_perc in_spec_perc Cpk ppm_defects -1 1 31.73 68.27 0.333 317’300 -2 2 4.55 95.45 0.667 45’500 -3 3 0.27 99.73 1.000 2’700 -4 4 0.01 99.99 1.333 100 -5 5 0.00 100.00 1.667 0 -6 6 0.00 100.00 2.000 0 Capability chart syringe_cpk &lt;- process.capability( syringe_xbar, breaks = 10, spec.limits = c(spec_min, spec_max), target = spec_tgt, digits = 2, print = FALSE, std.dev = syringe_sd ) The previous plot shows how to obtain the capability chart and the different statistical variables in a quick and direct way with the {qcc} package. A fine tuning of the forecast of the number of expected parts out of specification can be done with the parameter std.dev. This input value will be used in the probability distribution function. Different approaches can be considered: calculating the standard deviation within each subgroup or the standard deviation of the entire population and also correcting the standard deviation dividing by n or by n - 1. In this example we re-use the standard deviation calculated on the entire set of datapoints as the group is small but for a case with more data it would be interesting to use the subgroups that tend to give smaller standard deviations. The Quality Assistant has a much more clear idea where the process stands now. There’s a realignment of the process required of at least 0.1 mm. This may be at the limit of the process capability itself not to mention the uncertainty of measurement. Before over reacting he decides to look into the historical data and talk with the operators. Once it becomes clear this is a recurring pattern he will start looking into how to make such adjustment. I-MR charts This final unit of the book requires prior knowledge in creating R functions and reading code from existing R functions. It develops the topic of the development of custom functions for summary statistics and timeseries plotting. These functions are available on the book companion package {industRial} for exploration and further development. They don’t pretend to be used as such for real life applications as for that we recommend the functions from the package {qcc} presented before. In previous cases we’re already presented how to modify functions from other packages and here the objective here is to explore the creation of new functions from scratch. We’re not presenting here the complete code for each function but instead encourage it be checked with the R functionality for function exploration. In R programming books and forums we see often the recommendation to read R source code and we can only support it as an excellent way to develop our skill set. Lets start with the simple function off_spec() that calculates the percentage of parts out of specification given the specification limits, the process mean and standard deviation. This function was presented in the previous case study and since it is loaded in memory we can read its content with the R function body(): body(off_spec) { round(100 - ((stats::pnorm(UCL, mean, sd) - stats::pnorm(LCL, mean, sd)) * 100), 2) } we can see that it uses simple functions from the package {stats}. We can also explicitly request to see the arguments it takes with dput(formals()): dput(formals(off_spec)) as.pairlist(alist(UCL = , LCL = , mean = , sd = )) and for a complete review we can open the function help page with: ?off_spec Lets give some data and use the function: off_spec(0.981, 0.819, 0.943, 0.019) [1] 2.28 we get 2.28% parts out of spec. We’ll see this calculation in action in a moment. Process statistics tablet_weight &lt;- tablet_weight %&gt;% janitor::clean_names(case = &quot;snake&quot;) tablet_weight %&gt;% head(3) %&gt;% kable() part_id weight_target_value weight_value 1001 0.9 0.84556 1002 0.9 0.91444 1003 0.9 0.90111 We’re now going to use the function process_stats() to calculate several statistical data for this dataset. This function returns a high number of outputs is large as we can see below: weight_statistics_data &lt;- process_stats(tablet_weight, 10) names(weight_statistics_data) [1] &quot;part_id&quot; &quot;weight_target_value&quot; &quot;weight_value&quot; [4] &quot;part_spec_percent&quot; &quot;spec_min&quot; &quot;spec_max&quot; [7] &quot;weight_mean&quot; &quot;weight_sd&quot; &quot;weight_out_perc&quot; [10] &quot;Cpk&quot; &quot;weight_MR&quot; &quot;weight_MR_mean&quot; [13] &quot;MR_max&quot; &quot;R_out_limits&quot; &quot;I_LCL&quot; [16] &quot;I_UCL&quot; &quot;weight_out_limits&quot; Exploring the details on how the function calculates all the process statistics we have: body(process_stats) { process_data &lt;- data %&gt;% dplyr::filter(!is.na(.data$weight_value), .data$weight_value &gt;= 0) %&gt;% dplyr::group_by(.data$weight_target_value) %&gt;% dplyr::mutate(part_spec_percent = part_spec_percent, spec_min = .data$weight_target_value - .data$weight_target_value * part_spec_percent/100, spec_max = .data$weight_target_value + .data$weight_target_value * part_spec_percent/100, weight_mean = mean(.data$weight_value), weight_sd = stats::sd(.data$weight_value)) process_data &lt;- process_data %&gt;% dplyr::mutate(weight_out_perc = industRial::off_spec(process_data$spec_max, process_data$spec_min, process_data$weight_mean, process_data$weight_sd), Cpk = industRial::process_Cpk(process_data$spec_max, process_data$spec_min, process_data$weight_mean, process_data$weight_sd), weight_MR = abs(.data$weight_value - dplyr::lag(.data$weight_value))) process_data &lt;- process_data %&gt;% dplyr::mutate(weight_MR_mean = mean(process_data$weight_MR, na.rm = TRUE)) process_data &lt;- process_data %&gt;% dplyr::mutate(MR_max = 3.688 * process_data$weight_MR_mean) process_data &lt;- process_data %&gt;% dplyr::mutate(R_out_limits = dplyr::if_else(condition = process_data$weight_MR &gt; process_data$MR_max, process_data$weight_MR, NA_real_), I_LCL = round((process_data$weight_mean - 2.66 * process_data$weight_MR_mean), 2), I_UCL = round((process_data$weight_mean + 2.66 * process_data$weight_MR_mean), 2)) process_data &lt;- process_data %&gt;% dplyr::mutate(weight_out_limits = dplyr::if_else(condition = (.data$weight_value &gt; process_data$I_UCL | .data$weight_value &lt; process_data$I_LCL), .data$weight_value, false = NA_real_)) } This being done we can now convert this data into an easy readable format for reporting of for a future integration in a shiny app for example. We’re exploring now the package {gt} that has a specific very neat look rather different from the {kable} package that has been the most used until now. process_stats_table(weight_statistics_data) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #jiwsvkghsy .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #jiwsvkghsy .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jiwsvkghsy .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #jiwsvkghsy .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #jiwsvkghsy .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jiwsvkghsy .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jiwsvkghsy .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #jiwsvkghsy .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #jiwsvkghsy .gt_column_spanner_outer:first-child { padding-left: 0; } #jiwsvkghsy .gt_column_spanner_outer:last-child { padding-right: 0; } #jiwsvkghsy .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #jiwsvkghsy .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #jiwsvkghsy .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #jiwsvkghsy .gt_from_md > :first-child { margin-top: 0; } #jiwsvkghsy .gt_from_md > :last-child { margin-bottom: 0; } #jiwsvkghsy .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #jiwsvkghsy .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #jiwsvkghsy .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jiwsvkghsy .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #jiwsvkghsy .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jiwsvkghsy .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #jiwsvkghsy .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #jiwsvkghsy .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jiwsvkghsy .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jiwsvkghsy .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #jiwsvkghsy .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jiwsvkghsy .gt_sourcenote { font-size: 90%; padding: 4px; } #jiwsvkghsy .gt_left { text-align: left; } #jiwsvkghsy .gt_center { text-align: center; } #jiwsvkghsy .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #jiwsvkghsy .gt_font_normal { font-weight: normal; } #jiwsvkghsy .gt_font_bold { font-weight: bold; } #jiwsvkghsy .gt_font_italic { font-style: italic; } #jiwsvkghsy .gt_super { font-size: 65%; } #jiwsvkghsy .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Process Summary Statistics Variable Value Unit Weight mean 0.94000 g Spec target 0.90000 g Spec min 0.81000 g Spec max 0.99000 g Out of spec 0.71000 % Cpk 0.81785 Sample size 137 parts Individual chart chart_I(weight_statistics_data) Once the data set is available we’re feeding it into the chart_I() function. This is another function from the package that can be used as a starting point for explorations in SPC charts. It is built using {ggplot2} and another examples can be seen in the tutorials section. Moving range chart chart_IMR(weight_statistics_data) The companion of the I chart is the MR chart, where MR stands for moving range. Capability chart (custom) chart_Cpk(weight_statistics_data) and this final chart presented a process capability histogram obtained with the function chart_Cpk(). References "],["contents.html", "Index", " Index Subject Unit Functions Datasets DFSS Pareto analysis Pareto chart qicharts2::paretochart dial_control Root cause analysis ishikawa diagram qcc::cause.and.effect Correlations Matrix perfume_experiment Tileplot ggplot2::geom_tile Clustering Network plot ggraph::ggraph perfume_experiment MSA Calibration Bias plot ggplot2::geom_smooth juice_drymatter Bias report Precision Gage r&amp;R SixSigma::ss.rr tablet_thickness Gage acceptance Uncertainty Uncertainty tablet_thickness DOE Direct comparisons Histogram ggplot2::geom_histogram pet_delivery t-test one sample stats::t.test Normality plot ggplot2::geom_qq F test stats::var.test Levene test car::leveneTest Statistical modeling Linear models stats::lm ebike_hardening Contrasts treatment stats::contrasts Predict stats::predict Model augment broom::augment Timeseries plot Autocorrelation test car::durbinWatsonTest Normality test stats::shapiro.test Residuals-Fit plot Homocedasticity stats::bartlett.test Standard Residuals-Fit plot Outliers test car::outlierTest Cooks distance R-squared base::summary()$r.squared Effects significance Anova stats::aov Pairwise comparison stats::TukeyHSD Least significant difference agricolae::LSD.test Interactions Model formulae stats::formula solarcell_output Interaction plot stats:interaction.plot Residual standard error Residuals summary stats::plot.lm Anova with interactions stats::anova Covariance Correlation test stats::cor.test solarcell_fill Analysis of covariance stats::aov General designs Factorial design DoE.base::fac.design juice_drymatter Main effects plots Interactions plots (custom) ggplot2::geom_errorbar Formula expansion industRial::formula_expansion Anova 3rd level interactions Two level designs Coding levels stats::relevel pet_doe Interaction plots with SE RcmdrMisc::plotMeans Adjusted R-squared broom::glance battery_charging Coding inputs Coding prediction Perspective plot graphics::persp Single replicate Effects normal plot car::qqPlot Effects contribution table broom::tidy SPC Xbar-R charts Mean chart qcc::qcc syringe_diameter Range chart qcc::qcc Warning limits qcc::limits.xbar Cpk charts Off specification industRial::off_spec syringe_diameter Process capability industRial::process_Cpk Sigma conversion table Capability chart qcc::process.capability I-MR charts Process statistics tablet_weight Individual chart industRial::chart_I Moving range chart industRial::chart_IMR Capability chart (custom) industRial::chart_Cpk "],["glossary.html", "Glossary Statistics DFSS MSA DOE SPC", " Glossary Statistics Statistic concepts are picked up and applied through the Cases Studies on a needed basis. To get a better understanding of how they fit together we are reminding below some definitions coming from Yakir (2011). For a deep and comprehensive course on statistics we recommend the free online kahn academy courses. Notation conventions The arithmetic mean of a series of values x1, x2, …, xn is often denoted by placing an “overbar” over the symbol, e.g. \\(\\bar{x}\\) , pronounced “x bar.” Some commonly used symbols for sample statistics are: sample mean \\(\\bar{x}\\), sample standard deviation s. Some commonly used symbols for population parameters: population mean μ, population standard deviation σ. Random variables are usually written in upper case roman letters: \\(X, Y\\), etc. Particular realizations of a random variable are written in corresponding lower case letters. For example, x1, x2, …, xn could be a sample corresponding to the random variable \\(X\\). A cumulative probability is formally written P(\\(X\\)≤x) to differentiate the random variable from its realization. Greek letters (e.g. θ, β) are commonly used to denote unknown parameters (population parameters). Placing a hat, or caret, over a true parameter denotes an estimator of it, e.g., \\(\\hat{θ}\\) is an estimator for θ. Descriptive statistics Statistic: A numerical characteristic of the data. A statistic estimates the corresponding population parameter. Population: The collection, or set, of all individuals, objects, or measurements whose properties are being studied. Sample: A portion of the population understudy. A sample is representative if it characterizes the population being studied. Frequency: The number of times a value occurs in the data. Relative Frequency: The ratio between the frequency and the size of data. \\(f / n\\) Median: A number that separates ordered data into halves. Mean: A number that measures the central tendency. A common name for mean is ‘average.’ Sample size: \\(n\\) Sample mean: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\sum_{x}(x\\times f_x / n)\\) Population size: \\(N\\) Population mean: \\(\\bar{\\mu} = \\frac{\\sum_{i=1}^{N}x_i}{N}\\) Variance: Mean of the squared deviations from the mean. Sample variance: \\(s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\frac{n}{n-1}\\sum_{x}((x-\\bar{x})^2\\times(f_x/n))\\) Standard Deviation: A number that is equal to the square root of the variance and measures how far data values are from their mean. Sample standard deviation: \\(\\sqrt[]{s^{2}}\\) Probability Random Variable: The probabilistic model for the value of a measurement, before the measurement is taken (e.g. Binomial, Poisson, Uniform, Exponential, Normal). It is denoted with Latin capitals \\(X, Y\\) and \\(Z\\) Expectation: The central value for a random variable. The expectation of the random variable X is marked by E(\\(X\\)). Variance: The (squared) spread of a random variable. The variance of the random variable X is marked by Var(\\(X\\)). Normal Random Variable: A bell-shaped distribution that is frequently used to model a measurement. The distribution is marked with X ~ Normal\\((\\mu,\\sigma^2)\\). Exponential Random Variable: probability distribution of the time between events (in a Poisson random measurement). It is denoted as X ∼ Exponential(λ) where λ is a parameter that characterizes the distribution and is called the rate of the distribution. Uniform Random Variable: used in order to model measurements that may have values in a given interval, with all values in this interval equally likely to occur. Noted as X ∼ Uniform(a, b). Standard Normal Distribution: The Normal(0,1). The distribution of standardized normal measurement. Percentile: Given a percent p · 100% (or a probability p), the value x is the percentile of a random variable X if it satisfies the equation P\\((X ≤ x) = p\\). Random Sample: The probabilistic model for the values of a measurements in the sample, before the measurement is taken. Sampling Distribution: The distribution of a random sample. Sampling Distribution of a Statistic: A statistic is a function of the data; i.e. a formula applied to the data. The statistic becomes a random variable when the formula is applied to a random sample. The distribution of this random variable, which is inherited from the distribution of the sample, is its sampling distribution. Sampling Distribution of the Sample Average: The distribution of the sample average, considered as a random variable. The Law of Large Numbers: A mathematical result regarding the sampling distribution of the sample average. States that the distribution of the av- erage of measurements is highly concentrated in the vicinity of the expec- tation of a measurement when the sample size is large. The Central Limit Theorem: A mathematical result regarding the sampling distribution of the sample average. States that the distribution of the average is approximately Normal when the sample size is large. (note: the central limit theorem is a key notion for understanding industrial measurement and its consequences will be applied in most case studies) Expectation of the sample average: the expectation of the sample mean is equal to the theoretical expectation of its components \\(E(\\bar{X}) = E(X)\\) Variance of the sample average: the variance of the sample average is equal to the variance of each of the components, divided by the sample size \\(Var(\\bar X) = Var(X)/n\\) Expectation for an uniform distribution: \\(E(X) = \\frac{a+b}{2}\\) Variance for an uniform distribution: \\(Var(X) = \\frac{(b-a)^2}{12}\\) Expectation for a normal distribution: \\(E(X) = \\mu\\) Variance for a normal distribution: \\(Var(X) = \\sigma^2\\) Expectation for an exponential distribution: \\(E(X) = 1/\\lambda\\) Variance for an exponential distribution: \\(Var(X) = 1\\lambda^2\\) Statistical Inference Statistical Inference: Methods for gaining insight regarding the population parameters from the observed data. Point Estimation: An attempt to obtain the best guess of the value of a population parameter. An estimator is a statistic that produces such a guess. The estimate is the observed value of the estimator. Confidence Interval: An interval that is most likely to contain the population parameter. The confidence level of the interval is the sampling probability that the confidence interval contains the parameter value. Hypothesis Testing: A method for determining between two hypothesis, with one of the two being the currently accepted hypothesis. A determination is based on the value of the test statistic. The probability of falsely rejecting the currently accepted hypothesis is the significance level of the test. Comparing Samples: Samples emerge from different populations or under different experimental conditions. Statistical inference may be used to compare the distributions of the samples to each other. Bias: The difference between the expectation of the estimator and the value of the parameter. An estimator is unbiased if the bias is equal to zero. Otherwise, it is biased. Mean Square Error (MSE): A measure of the concentration of the distribu- tion of the estimator about the value of the parameter. The mean square error of an estimator is equal to the sum of the variance and the square of the bias. If the estimator is unbiased then the mean square error is equal to the variance. Confidence Level: The sampling probability that random confidence intervals contain the parameter value. The confidence level of an observed interval indicates that it was constructed using a formula that produces, when applied to random samples, such random intervals. Null Hypothesis (\\(H0\\)): A sub-collection that emerges in response to the situation when the phenomena is absent. The established scientific theory that is being challenged. The hypothesis which is worse to erroneously reject. Alternative Hypothesis (\\(H1\\)): A sub-collection that emerges in response to the presence of the investigated phenomena. The new scientific theory that challenges the currently established theory. Test Statistic: A statistic that summarizes the data in the sample in order to decide between the two alternative. Rejection Region: A set of values that the test statistic may obtain. If the observed value of the test statistic belongs to the rejection region then the null hypothesis is rejected. Otherwise, the null hypothesis is not rejected. Type I Error: The null hypothesis is correct but it is rejected by the test. Type II Error: The alternative hypothesis holds but the null hypothesis is not rejected by the test. Significance Level: The probability of a Type I error. The probability, computed under the null hypothesis, of rejecting the null hypothesis. The test is constructed to have a given significance level. A commonly used significance level is 5%. Statistical Power: The probability, computed under the alternative hypothesis, of rejecting the null hypothesis. The statistical power is equal to 1 minus the probability of a Type II error. \\(p\\)-value: is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct. A very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis. A form of a test statistic. It is associated with a specific test statistic and a structure of the rejection region. The p-value is equal to the significance level of the test in which the observed value of the statistic serves as the threshold. Linear models Regression: Relates different variables that are measured on the same sample. Regression models are used to describe the effect of one of the variables on the distribution of the other one. The former is called the explanatory variable and the later is called the response. R-squared: is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). R-squared gives an indication of the quality of the model and can be calculated with the formula \\(R^2 = 1 - \\frac{VAR(res)}{VAR(y)}\\). In a linear least squares regression with an intercept term and a single explanator, this is also equal to the squared Pearson correlation coefficient of the dependent variable y and explanatory variable x. Is also called Coefficient of Determination. Residuals standard error: an unbiased estimator of the magnitude of a typical residual. Can be calculated with the formula \\(RSE = \\sqrt{\\frac{\\sum{residuals^2}}{Df}}\\) where Df is the number of Degrees of freedom. Sometimes is also called Root Mean Squared Error. DFSS One way of summarizing the Six Sigma framework is presented below in a step by step approach with definitions. Each steps consists of an analysis of the product development and production process that progressively refines the final product specifications. For a more detailed description we recommend reviewing the comprehensive Six Sigma certification reference book by Roderik A.Munro and J.Zrymiak (2015). Voice of Customer 1. Product brief List of the product features expected by the customer (internal or external), including qualitative indication of the acceptance limits. 2. Functional analysis Translation of the product attributes into lower level functions including interactions between product components and requirements induced by each component on the others. 3. Failure modes and effects analysis (FMEA) List of critical product features with failure causes, effects, detection and action plans, rated and sorted by criticality. 4. Product specifications and parts drawings Implementation of the product components into unique formulations and drawings including detailed values and tolerances of it characteristics (physical, chemical or electric or others). Voice of Process 1. Process mapping A visual diagram of the production process with inputs and outputs for each step. 2. Process FMEA List of critical production process steps with failure causes, effects, detection and action plans, rated and sorted by criticality. 3. Quality Control plan List of control points including measurement method, sample size, frequency and acceptance criteria. When needed, critical control points are handled by Statistical Process Control (SPC) 4. Measurement system analysis A thorough assessment of a measurement process, and typically includes a specially designed experiment that seeks to identify the components of variation in that measurement process. 5. Process capability analysis Comparison of the variability of a production process with its engineered specifications. MSA It is a fact that different communities utilize different methodologies and terminologies on the domain of measurement uncertainty. Unfortunately these differences are still too often overlapping, see J E Muelaner (2015) for detailed comparison. In our text we opt for the industry terminology, in particular to the norm ISO 5725, the practical application guides from Automotive Industry Action Group (2010) and some articles on Minitab (2019b) which itself is based on the AIAG guidelines. Variance components assess the amount of variation contributed by each source of measurement error, plus the contribution of part-to-part variability. The sum of the individual variance components equals the total variation. total gage r&amp;R: the sum of the repeatability and the reproducibility variance components. part: The variation that comes from the parts, with 5 levels in this case. operator: The variation that comes from the operators, with 3 levels in this case. replicates, n: number of replications corresponding to the number of times each part is measured by each operator. repeatability (or error, or residuals): The variation that is not explained by part, operator, or the operator and part interaction. It represents how much variability is caused by the measurement device (the same operator measures the same part many times, using the same gage). The repeatability can be measured directly from the Anova table from the residual mean squares. reproducibility: how much variation is caused by the differences between operators (different operators measure the same part many times, using the same gage). operators: the operators part of the reproducibility is the operators variation minus the interaction divided by the number of different parts times the replicates (zero if negative). parts:operators: The variation that comes from the operator and part interaction. An interaction exists when an operator measures different parts differently. The interaction part of of the reproducibility is the interaction minus the repeatability divided by the number of replicates (zero if negative). part-to-part: the variability due to different parts. Ideally, very little should be due to repeatability and reproducibility. Differences between parts should account for most of the variability (when the %Contribution from part-to-part variation is high, the measurement system can reliably distinguish between parts). The sum of the individual variance components equals the total variation. Accuracy/Uncertainty: Combination of precision and trueness. In the ISO 5725 both terms are equivalent. Precision: Combination of repeatability and reproducibility Trueness: difference between the mean of many measurements and the reference value. In ISO 5725 the term bias has been replaced by trueness. DOE Below key definitions from Montgomery (2012), complemented with Wikipedia article details on the same topics. Randomization: both the allocation of the experimental material and the order in which the individual runs of the experiment are to be performed are randomly determined. Run: unique combination of the input factors. Replicate: independent repeat of a run. Trials: total quantity of tests in a DOE corresponding to the multiplication of the runs by the replicates. Experiment: series of runs. Factorial design: in each complete trial or replicate of the experiment all possible combinations of the levels of the factors are investigated. Crossed factors: factors arranged in a factorial design. Coded variable: the \\(\\pm\\) 1 coding for the low and high levels of the factors. Coded variables are very effective for determining the relative size of factor effects. In almost all situations, the coded unit analysis is preferable. Contrast: a linear combination of parameters in the form \\(\\tau=\\sum_{i=1}^{a}c_i\\mu_i\\) where the contrast constants \\(c_1,c_2, ..., c_a\\) sum to zero; that is, \\(\\sum_{i=1}^{a}c_i=0\\). Orthogonal contrasts: two contrasts with coefficients \\({c_i}\\) and \\({d_i}\\) are orthogonal if \\(\\sum_{i=1}^{a}c_id_i = 0\\). In a balanced one-way analysis of variance, using orthogonal contrasts has the advantage of completely partitioning the treatment sum of squares into non-overlapping additive components that represent the variation due to each contrast. Contrasts then allow for the comparison between the different means. Sparsity of effects principle: states that most systems are dominated by some of the main effects and low-order interactions, and most high-order interactions are negligible. SPC Process capability: is the ability of a manufacturing process to produce an output within the product specification limits. Process Capability Index: a statistical measure of the process capability. Different indexes have been defined: Cp, Cpk, Cpm, Cpkm. References "],["references.html", "References", " References A good mastership of the vast domain of Industrial Data Science can take several years and can only be obtained by a strong combination of theory and practice. As mentioned in the introduction chapter, our book is focused on the practice and in this bibliography we find some the necessary supporting theory. The list below collects websites, books and articles referenced throughout this book. It is a curated set of some of the most relevant works available today in Six Sigma, Statistics, Data Science and programming with R. "],["imprint.html", "Imprint", " Imprint Many packages are available for editing documentation, from notes to blogs up to complete websites. In this book we’ve opted to use the R package {Bookdown} from Yihui Xie further customized with a layout developed by Matthew J. C. Crump. An important aspect to ensure reproducibility of the examples along the time and between users is to have the same programming setup. We’re showing below our setup at the time of rendering the book. devtools::session_info()[[1]] ## setting value ## version R version 4.1.1 (2021-08-10) ## os Ubuntu 20.04.3 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Europe/Zurich ## date 2021-10-23 Disclaimer This book presents a variety of software tools and recommended approaches for industrial data analysis. It is incumbent upon the user to execute judgment in their use. The author does not provide any guarantee, expressed or implied, with regard to the general or specific applicability of the software, the range of errors that may be associated with it, or the appropriateness of using them in any subsequent calculation, design, or decision process. The author accepts no responsibility for damages, if any, suffered by any reader or user of this handbook as a result of decisions made or actions taken on information contained therein. License This book and its companion package are made available under a GPLv3 license granting end users the freedom to run, study, share, and modify the software. "]]
